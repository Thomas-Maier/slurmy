{
    "docs": [
        {
            "location": "/",
            "text": "SLURMY - Special handLer for Universal Running of Multiple jobs, Yes!\n\n\nSlurmy is a general batch submission module, which allows to define very general jobs to be run on batch system setups on linux computing clusters. Currently, only slurm is supported as backend, but further backends can easily be added. The definition of the job execution is done with a general shell execution script, as is used by most batch systems. In addition to the batch definition, jobs can also be dynamically executed locally, which allows for an arbitrary combination of batch and local jobs.\n\n\nRecommended Setup\n\n\nClone the latest stable tag or master branch locally:\n\n\ngit clone https://github.com/Thomas-Maier/slurmy.git\n\n\nMake sure to add the directory in which you cloned slurmy to PYTHONPATH, and the slurmy folder to PATH:\n\n\nexport PYTHONPATH=$PWD:$PYTHONPATH\n\n\nexport PATH=$PWD/slurmy:$PATH\n\n\nThis will make python aware of the slurmy module and you'll be able to execute the slurmy executable.\n\n\nAlso, take a look at the \nslurmy config setup\n.",
            "title": "Slurmy"
        },
        {
            "location": "/#slurmy-special-handler-for-universal-running-of-multiple-jobs-yes",
            "text": "Slurmy is a general batch submission module, which allows to define very general jobs to be run on batch system setups on linux computing clusters. Currently, only slurm is supported as backend, but further backends can easily be added. The definition of the job execution is done with a general shell execution script, as is used by most batch systems. In addition to the batch definition, jobs can also be dynamically executed locally, which allows for an arbitrary combination of batch and local jobs.",
            "title": "SLURMY - Special handLer for Universal Running of Multiple jobs, Yes!"
        },
        {
            "location": "/#recommended-setup",
            "text": "Clone the latest stable tag or master branch locally:  git clone https://github.com/Thomas-Maier/slurmy.git  Make sure to add the directory in which you cloned slurmy to PYTHONPATH, and the slurmy folder to PATH:  export PYTHONPATH=$PWD:$PYTHONPATH  export PATH=$PWD/slurmy:$PATH  This will make python aware of the slurmy module and you'll be able to execute the slurmy executable.  Also, take a look at the  slurmy config setup .",
            "title": "Recommended Setup"
        },
        {
            "location": "/howto/",
            "text": "General Usage\n\n\nYou can just write a piece of python code that imports the required slurmy classes (e.g. \nJobHandler\n and the backend class of your choice), defines jobs and calls the job submission. Job execution definitions can either be provided by already written batch shell scripts, or by defining the content of the shell script directly in your python code. For both cases, arguments that should be passed upon the execution to the scripts can also be specified.\n\n\n\n\nWhat you want to do before doing anything else\n\n\nYou can (and should) specify a slurmy config file, which defines your default configuration of relevant slurmy properties. In particular you can define which batch system backend you want to use and how it should be configured. This safes you from having to specify this every single time you want to use slurmy.\n\n\nYou just need to create a file \n~/.slurmy\n with this content (which you might want to modify):\n\n\nbookkeeping = ~/.slurmy_bookkeeping\nworkdir = ./\nbackend = Slurm\neditor = emacs -nw\n## Slurm backend options\nSlurm.partition = lsschaile\n#Slurm.clusters = \n#Slurm.qos = \n#Slurm.exclude =\n#Slurm.mem =\n#Slurm.time =\n#Slurm.export =\n\n\n\n\n\n\nSnapshots\n\n\nBy default, slurmy will do bookkeeping of past \nJobHandler\n sessions. The information is stored in a json file, which is defined in the slurmy config. Snapshot making can be deactivated by passing the respective argument to \nJobHandler\n.\n\n\nSnapshot making is very useful, in particular if you want to make use of \ninteractive slurmy\n.\n\n\nSimple example\n\n\nfrom slurmy import JobHandler\n\n## Set up the JobHandler\njh = JobHandler()\n## Define the run script content\nrun_script = \"\"\"\necho \"hans\"\n\"\"\"\n## Add a job\njh.add_job(run_script = run_script)\n## Run all jobs\njh.run_jobs()\n\n\n\n\nWith explicit backend definition\n\n\nIf you don't create a slurmy config file specifying the batch system backend to use, or if you want to specify backends for each job, then you can do the following.\n\n\nfrom slurmy import JobHandler, Slurm\n\n## Set up the backend\nslurm = Slurm(partition = 'lsschaile')\n## Set up the JobHandler\njh = JobHandler(backend = slurm)\n## Define the run script content\nrun_script = \"\"\"\necho \"hans\"\n\"\"\"\n## Add a job\n### The backend can be individually set for each job\nslurm_job = Slurm(partition = 'lsschaile', mem = '6000mb')\njh.add_job(backend = slurm_job, run_script = run_script)\n## Run all jobs\njh.run_jobs()\n\n\n\n\nUsing an already existing script file (with optional arguments)\n\n\nYou can also just use an existing script file. In this case you just specify the file path as \nrun_script\n. You can also specify additional arguments to be passed to the run script.\n\n\nfrom slurmy import JobHandler\n\n## Set up the JobHandler\njh = JobHandler()\n## Specify path to run script on disk\nrun_script = '/path/to/run_script'\n## Additional arguments to be passed to the run script\nrun_args = 'hans horst'\n## Add a job\njh.add_job(run_script = run_script, run_args = run_args)\n## Run all jobs\njh.run_jobs()\n\n\n\n\nChaining Jobs with Tags\n\n\nJobs can be connected by adding tags and parent tags to them. Jobs with parent tags X,Y, and Z will only be executed if all jobs that have the tags X, Y, or Z have successfully finished.\n\n\nfrom slurmy import JobHandler\n\n## Set up the JobHandler\njh = JobHandler()\n## Define the run script content of job 1\nrun_script_1 = \"\"\"\nsleep 10\necho \"hans\"\n\"\"\"\n## Add job 1 with tag \"hans\"\njh.add_job(run_script = run_script_1, tags = 'hans')\n## Define the run script content of job 2\nrun_script_2 = \"\"\"\necho \"horst\"\n\"\"\"\n## Add job 2 with parent_tag \"hans\"\njh.add_job(run_script = run_script_2, parent_tags = 'horst')\n## Run all jobs\njh.run_jobs()\n\n\n\n\nCustom evaluations for success, finished, and post-execution\n\n\nBy default, the exitcode of the job (either taken from the local process or from the batch system bookkeeping) is taken to determine if it was successful or not. However, you can define a custom success condition by creating a dedicated class with __call__ defined. The function has to have exactly one argument, which is the config instance of the job. If \nsuccess_func\n was defined during add_job, the custom definition will be used instead of the default one during the success evaluation.\n\n\nThis example uses the predefined \nSuccessOutputFile\n class, which checks if the output associated to job exists on disk in order to evaluate whether if it succeeded or not.\n\n\nfrom slurmy import JobHandler, SuccessOutputFile\n\n## Set up success evaluation class which checks if the job's output exists or not\nsf = SuccessOutputFile()\n## Set up the JobHandler\njh = JobHandler(success_func = sf)\n## Define the run script content\nrun_script = \"\"\"\ntouch ~/hans.txt\n\"\"\"\n## Add a job, specifying the output of the job.\n##The success evaluation class can be specified for each job separately.\njh.add_job(run_script = run_script, output = '~/hans.txt', success_func = sf)\n## Run all jobs\njh.run_jobs()\n\n\n\n\nIn the same way as \nsuccess_func\n, you can also define \nfinished_func\n, to evaluate if a job is finished, or \npost_func\n, to define a post-processing which will be done locally after the job's success evaluation was done.\n\n\nRegarding class definitions\n\n\nDue to technically reasons connected to the \nsnapshot feature\n, your custom class definition must be known to python on your machine. The best way to ensure that is to make the definition known to python via PYTHONPATH. In principle you can just use a local function definition instead of a callable class if you don't want to use the snapshot feature. However, it is highly recommended to make use of it.",
            "title": "HowTo"
        },
        {
            "location": "/howto/#general-usage",
            "text": "You can just write a piece of python code that imports the required slurmy classes (e.g.  JobHandler  and the backend class of your choice), defines jobs and calls the job submission. Job execution definitions can either be provided by already written batch shell scripts, or by defining the content of the shell script directly in your python code. For both cases, arguments that should be passed upon the execution to the scripts can also be specified.",
            "title": "General Usage"
        },
        {
            "location": "/howto/#what-you-want-to-do-before-doing-anything-else",
            "text": "You can (and should) specify a slurmy config file, which defines your default configuration of relevant slurmy properties. In particular you can define which batch system backend you want to use and how it should be configured. This safes you from having to specify this every single time you want to use slurmy.  You just need to create a file  ~/.slurmy  with this content (which you might want to modify):  bookkeeping = ~/.slurmy_bookkeeping\nworkdir = ./\nbackend = Slurm\neditor = emacs -nw\n## Slurm backend options\nSlurm.partition = lsschaile\n#Slurm.clusters = \n#Slurm.qos = \n#Slurm.exclude =\n#Slurm.mem =\n#Slurm.time =\n#Slurm.export =",
            "title": "What you want to do before doing anything else"
        },
        {
            "location": "/howto/#snapshots",
            "text": "By default, slurmy will do bookkeeping of past  JobHandler  sessions. The information is stored in a json file, which is defined in the slurmy config. Snapshot making can be deactivated by passing the respective argument to  JobHandler .  Snapshot making is very useful, in particular if you want to make use of  interactive slurmy .",
            "title": "Snapshots"
        },
        {
            "location": "/howto/#simple-example",
            "text": "from slurmy import JobHandler\n\n## Set up the JobHandler\njh = JobHandler()\n## Define the run script content\nrun_script = \"\"\"\necho \"hans\"\n\"\"\"\n## Add a job\njh.add_job(run_script = run_script)\n## Run all jobs\njh.run_jobs()",
            "title": "Simple example"
        },
        {
            "location": "/howto/#with-explicit-backend-definition",
            "text": "If you don't create a slurmy config file specifying the batch system backend to use, or if you want to specify backends for each job, then you can do the following.  from slurmy import JobHandler, Slurm\n\n## Set up the backend\nslurm = Slurm(partition = 'lsschaile')\n## Set up the JobHandler\njh = JobHandler(backend = slurm)\n## Define the run script content\nrun_script = \"\"\"\necho \"hans\"\n\"\"\"\n## Add a job\n### The backend can be individually set for each job\nslurm_job = Slurm(partition = 'lsschaile', mem = '6000mb')\njh.add_job(backend = slurm_job, run_script = run_script)\n## Run all jobs\njh.run_jobs()",
            "title": "With explicit backend definition"
        },
        {
            "location": "/howto/#using-an-already-existing-script-file-with-optional-arguments",
            "text": "You can also just use an existing script file. In this case you just specify the file path as  run_script . You can also specify additional arguments to be passed to the run script.  from slurmy import JobHandler\n\n## Set up the JobHandler\njh = JobHandler()\n## Specify path to run script on disk\nrun_script = '/path/to/run_script'\n## Additional arguments to be passed to the run script\nrun_args = 'hans horst'\n## Add a job\njh.add_job(run_script = run_script, run_args = run_args)\n## Run all jobs\njh.run_jobs()",
            "title": "Using an already existing script file (with optional arguments)"
        },
        {
            "location": "/howto/#chaining-jobs-with-tags",
            "text": "Jobs can be connected by adding tags and parent tags to them. Jobs with parent tags X,Y, and Z will only be executed if all jobs that have the tags X, Y, or Z have successfully finished.  from slurmy import JobHandler\n\n## Set up the JobHandler\njh = JobHandler()\n## Define the run script content of job 1\nrun_script_1 = \"\"\"\nsleep 10\necho \"hans\"\n\"\"\"\n## Add job 1 with tag \"hans\"\njh.add_job(run_script = run_script_1, tags = 'hans')\n## Define the run script content of job 2\nrun_script_2 = \"\"\"\necho \"horst\"\n\"\"\"\n## Add job 2 with parent_tag \"hans\"\njh.add_job(run_script = run_script_2, parent_tags = 'horst')\n## Run all jobs\njh.run_jobs()",
            "title": "Chaining Jobs with Tags"
        },
        {
            "location": "/howto/#custom-evaluations-for-success-finished-and-post-execution",
            "text": "By default, the exitcode of the job (either taken from the local process or from the batch system bookkeeping) is taken to determine if it was successful or not. However, you can define a custom success condition by creating a dedicated class with __call__ defined. The function has to have exactly one argument, which is the config instance of the job. If  success_func  was defined during add_job, the custom definition will be used instead of the default one during the success evaluation.  This example uses the predefined  SuccessOutputFile  class, which checks if the output associated to job exists on disk in order to evaluate whether if it succeeded or not.  from slurmy import JobHandler, SuccessOutputFile\n\n## Set up success evaluation class which checks if the job's output exists or not\nsf = SuccessOutputFile()\n## Set up the JobHandler\njh = JobHandler(success_func = sf)\n## Define the run script content\nrun_script = \"\"\"\ntouch ~/hans.txt\n\"\"\"\n## Add a job, specifying the output of the job.\n##The success evaluation class can be specified for each job separately.\njh.add_job(run_script = run_script, output = '~/hans.txt', success_func = sf)\n## Run all jobs\njh.run_jobs()  In the same way as  success_func , you can also define  finished_func , to evaluate if a job is finished, or  post_func , to define a post-processing which will be done locally after the job's success evaluation was done.",
            "title": "Custom evaluations for success, finished, and post-execution"
        },
        {
            "location": "/howto/#regarding-class-definitions",
            "text": "Due to technically reasons connected to the  snapshot feature , your custom class definition must be known to python on your machine. The best way to ensure that is to make the definition known to python via PYTHONPATH. In principle you can just use a local function definition instead of a callable class if you don't want to use the snapshot feature. However, it is highly recommended to make use of it.",
            "title": "Regarding class definitions"
        },
        {
            "location": "/interactive_slurmy/",
            "text": "You can use the \nslurmy\n executable to start an interactive slurmy session, which allows to interact with past JobHandler sessions or start new ones.\n\n\nUsage from \nslurmy --help\n:\n\n\nusage: slurmy [-h] [-p PATH] [-c CONFIG] [-t] [--debug]\n\nSlurmy interactive\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -p PATH, --path PATH  Path to the base folder of an existing JobHandler\n                        session. Directly loads the JobHandler as \"jh\".\n  -c CONFIG, --config CONFIG\n                        Path to a job configuration file.\n  -t                    Switch to start in test mode.\n  --debug               Run in debugging mode.\n\n\n\n\nIf you prefer to use python2 (not recommended), you can also run the \nslurmy2\n executable.\n\n\nIf no argument is passed to the slurmy executable, it tries to load the latest session according to the bookkeeping and load it as \njh\n.\n\n\nExample usage\n\n\nIn general you can do everything in interactive slurmy that you can also do in python file which handles your job definition. On top of that you can easily inspect and manipulate an already existing \nJobHandler\n session. Just executing \nslurmy\n will bring up the latest \nJobHandler\n session:\n\n\nIn [1]: jh\nOut[1]: Azathoth_1530051215\n\n\n\n\nEvery \nJobHandler\n has a member \njobs\n which keeps track of all it's attached jobs:\n\n\nIn [2]: jh.jobs\nOut[2]: \nJob \"hans\": CONFIGURED\n------------\nCONFIGURED(1)\n\n\n\n\nAs you can see, the \nJobHandler\n in this case has one job named \"hans\", which is in the CONFIGURED state. Every job is attached as property to \nJobHandler\n.jobs, which provides a direct handle to access them:\n\n\nIn [3]: jh.jobs.hans\nOut[3]: \nJob \"hans\"\nLocal: False\nBackend: Slurm\nScript: /home/t/Thomas.Maier/testSlurmy/Azathoth_1530051215/scripts/hans\nStatus: CONFIGURED\n\n\n\n\nAlternatively, jobs can be accessed directly by name via the \nJobHandler\n itself:\n\n\nIn [4]: jh['hans']\nOut[4]: \nJob \"hans\"\nLocal: False\nBackend: Slurm\nScript: /home/t/Thomas.Maier/testSlurmy/Azathoth_1530051215/scripts/hans\nStatus: CONFIGURED\n\n\n\n\nJobHandler\n.jobs also has a \nstatus_\n property for every possible job status, which will print all jobs which currently are in this status:\n\n\nIn [5]: jh.jobs.status_CONFIGURED\nJob \"hans\": CONFIGURED\n\nIn [6]: jh.jobs.status_RUNNING\n\n\nIn [7]:\n\n\n\n\nIn this example, job \"hans\" is in CONFIGURED state so we can run the job submission (\nrun_jobs\n for continuous submission until finished or \nsubmit_jobs\n for a single submission cycle):\n\n\nIn [7]: jh.run_jobs()\nJobs processed (batch/local/all): (1/0/1)\n     successful (batch/local/all): (0/0/0)\n     failed (batch/local/all): (1/0/1)\nTime spent: 5.4 s\n\n\n\n\n(Note: the submission interval of \nrun_jobs\n is set to 5 seconds by default)\n\n\nThe job \"hans\" is now in FAILED state:\n\n\nIn [8]: jh.jobs\nOut[8]: \nJob \"hans\": FAILURE\n------------\nFAILURE(1)\n\n\n\n\nWe can access the log file of the job directly via it's dedicated property (which opens the log with less), in order to find out what went wrong:\n\n\nIn [9]: jh.jobs.hans.log\n\n\n\n\nUsually, you probably want to fix our job configuration setup to fix a systematic problem in the job's run script creation. However, you can edit the run script directly:\n\n\nIn [10]: jh.jobs.hans.edit_script()\n\n\n\n\nIf any of the jobs ended up in FAILED or CANCELLED state, they can be retried by passing \nretry = True\n to \nrun_jobs\n or \nsubmit_jobs\n:\n\n\nIn [11]: jh.run_jobs(retry = True)\nJobs processed (batch/local/all): (1/0/1)\n     successful (batch/local/all): (1/0/1)\nTime spent: 5.3 s\n\n\n\n\nThe job \"hans\" is now in SUCCESS state (from fixing the run script before retrying the job):\n\n\nIn [12]: jh.jobs\nOut[12]: \nJob \"hans\": SUCCESS\n------------\nSUCCESS(1)\n\n\n\n\nWhile the job management should be handled by the \nJobHandler\n, you can also run job commands directly:\n\n\nIn [13]: jh.jobs.hans.rerun()\n\nIn [14]: jh.jobs.hans.get_status()\nOut[14]: <Status.SUCCESS: 3>\n\n\n\n\nYou should also run \njh.check_status()\n to update the \nJobHandler\n job bookkeeping:\n\n\nIn [15]: jh.check_status()\nJobs (success/fail/all): (1/0/1)\n\n\n\n\nHowever, it's likely that running jobs directly screws up the bookkeeping.\n\n\nHave a look at the \nJobHandler\n and \nJob\n documentation to see what you execute in interactive slurmy.\n\n\nJob configuration file\n\n\nThe job definition file passed with \n-c\n is a convenient way to make job definitions. Inside the slurmy session, all necessary imports, like JobHandler and the backend classes, are already provided. This allows for skimmed down JobHandler setups that then can be further interacted with (you can omit import statements). As long as your definition file is \"flat\" (no encapsulated definitions), i.e. like the examples given in the \nHowTo\n section, you can pass it to interactive slurmy.\n\n\nInteractive slurmy functions\n\n\nThe interactive slurmy session also defines a couple of functions.\n\n\nlist_sessions\n\n\nlist_sessions()\n\n\n\n\nList all available slurmy session according to the central bookkeeping.\n\n\nload\n\n\nload(name)\n\n\n\n\nLoad a slurmy session by name.\n\n\n\n\nname\n Name of the slurmy session, as listed by list_sessions().\n\n\n\n\nReturns jobhandler associated to the session (JobHandler).\n\n\nload_path\n\n\nload_path(path)\n\n\n\n\nLoad a slurmy session by full path.\n\n\n\n\npath\n Full folder path of the slurmy session, as listed by list_sessions().\n\n\n\n\nReturns jobhandler associated to the session (JobHandler).\n\n\nload_latest\n\n\nload_latest()\n\n\n\n\nLoad the latest slurmy session according to central bookkeeping.\n\n\nReturns jobhandler associated to the session (JobHandler).",
            "title": "Interactive Slurmy"
        },
        {
            "location": "/interactive_slurmy/#example-usage",
            "text": "In general you can do everything in interactive slurmy that you can also do in python file which handles your job definition. On top of that you can easily inspect and manipulate an already existing  JobHandler  session. Just executing  slurmy  will bring up the latest  JobHandler  session:  In [1]: jh\nOut[1]: Azathoth_1530051215  Every  JobHandler  has a member  jobs  which keeps track of all it's attached jobs:  In [2]: jh.jobs\nOut[2]: \nJob \"hans\": CONFIGURED\n------------\nCONFIGURED(1)  As you can see, the  JobHandler  in this case has one job named \"hans\", which is in the CONFIGURED state. Every job is attached as property to  JobHandler .jobs, which provides a direct handle to access them:  In [3]: jh.jobs.hans\nOut[3]: \nJob \"hans\"\nLocal: False\nBackend: Slurm\nScript: /home/t/Thomas.Maier/testSlurmy/Azathoth_1530051215/scripts/hans\nStatus: CONFIGURED  Alternatively, jobs can be accessed directly by name via the  JobHandler  itself:  In [4]: jh['hans']\nOut[4]: \nJob \"hans\"\nLocal: False\nBackend: Slurm\nScript: /home/t/Thomas.Maier/testSlurmy/Azathoth_1530051215/scripts/hans\nStatus: CONFIGURED  JobHandler .jobs also has a  status_  property for every possible job status, which will print all jobs which currently are in this status:  In [5]: jh.jobs.status_CONFIGURED\nJob \"hans\": CONFIGURED\n\nIn [6]: jh.jobs.status_RUNNING\n\n\nIn [7]:  In this example, job \"hans\" is in CONFIGURED state so we can run the job submission ( run_jobs  for continuous submission until finished or  submit_jobs  for a single submission cycle):  In [7]: jh.run_jobs()\nJobs processed (batch/local/all): (1/0/1)\n     successful (batch/local/all): (0/0/0)\n     failed (batch/local/all): (1/0/1)\nTime spent: 5.4 s  (Note: the submission interval of  run_jobs  is set to 5 seconds by default)  The job \"hans\" is now in FAILED state:  In [8]: jh.jobs\nOut[8]: \nJob \"hans\": FAILURE\n------------\nFAILURE(1)  We can access the log file of the job directly via it's dedicated property (which opens the log with less), in order to find out what went wrong:  In [9]: jh.jobs.hans.log  Usually, you probably want to fix our job configuration setup to fix a systematic problem in the job's run script creation. However, you can edit the run script directly:  In [10]: jh.jobs.hans.edit_script()  If any of the jobs ended up in FAILED or CANCELLED state, they can be retried by passing  retry = True  to  run_jobs  or  submit_jobs :  In [11]: jh.run_jobs(retry = True)\nJobs processed (batch/local/all): (1/0/1)\n     successful (batch/local/all): (1/0/1)\nTime spent: 5.3 s  The job \"hans\" is now in SUCCESS state (from fixing the run script before retrying the job):  In [12]: jh.jobs\nOut[12]: \nJob \"hans\": SUCCESS\n------------\nSUCCESS(1)  While the job management should be handled by the  JobHandler , you can also run job commands directly:  In [13]: jh.jobs.hans.rerun()\n\nIn [14]: jh.jobs.hans.get_status()\nOut[14]: <Status.SUCCESS: 3>  You should also run  jh.check_status()  to update the  JobHandler  job bookkeeping:  In [15]: jh.check_status()\nJobs (success/fail/all): (1/0/1)  However, it's likely that running jobs directly screws up the bookkeeping.  Have a look at the  JobHandler  and  Job  documentation to see what you execute in interactive slurmy.",
            "title": "Example usage"
        },
        {
            "location": "/interactive_slurmy/#job-configuration-file",
            "text": "The job definition file passed with  -c  is a convenient way to make job definitions. Inside the slurmy session, all necessary imports, like JobHandler and the backend classes, are already provided. This allows for skimmed down JobHandler setups that then can be further interacted with (you can omit import statements). As long as your definition file is \"flat\" (no encapsulated definitions), i.e. like the examples given in the  HowTo  section, you can pass it to interactive slurmy.",
            "title": "Job configuration file"
        },
        {
            "location": "/interactive_slurmy/#interactive-slurmy-functions",
            "text": "The interactive slurmy session also defines a couple of functions.",
            "title": "Interactive slurmy functions"
        },
        {
            "location": "/interactive_slurmy/#list_sessions",
            "text": "list_sessions()  List all available slurmy session according to the central bookkeeping.",
            "title": "list_sessions"
        },
        {
            "location": "/interactive_slurmy/#load",
            "text": "load(name)  Load a slurmy session by name.   name  Name of the slurmy session, as listed by list_sessions().   Returns jobhandler associated to the session (JobHandler).",
            "title": "load"
        },
        {
            "location": "/interactive_slurmy/#load_path",
            "text": "load_path(path)  Load a slurmy session by full path.   path  Full folder path of the slurmy session, as listed by list_sessions().   Returns jobhandler associated to the session (JobHandler).",
            "title": "load_path"
        },
        {
            "location": "/interactive_slurmy/#load_latest",
            "text": "load_latest()  Load the latest slurmy session according to central bookkeeping.  Returns jobhandler associated to the session (JobHandler).",
            "title": "load_latest"
        },
        {
            "location": "/classes/JobHandler/",
            "text": "JobHandler\n\n\nJobHandler(name=None, backend=None, work_dir='', local_max=0, is_verbose=False, success_func=None, finished_func=None, max_retries=0, theme=<Theme.Lovecraft: 1>, run_max=None, do_snapshot=True, use_snapshot=False, description=None, wrapper=None)\n\n\n\n\nMain handle to setup and submit jobs. Internally stores most information in the \nJobHandlerConfig\n class, which is stored on disk as a snapshot of the \nJobHandler\n session.\n\n\n\n\nname\n Name of the \nJobHandler\n. Defines the base directory name of the session.\n\n\nbackend\n Default backend instance used for the job setup.\n\n\nwork_dir\n Path where the base directory is created.\n\n\nlocal_max\n Maximum number of local jobs that will be submitted at a time.\n\n\nis_verbose\n Increase verbosity of shell output.\n\n\nsuccess_func\n Default success function used for the job setup.\n\n\nfinished_func\n Default finished function used for the job setup.\n\n\nmax_retries\n Maximum number of retries that are attempted for failing jobs.\n\n\ntheme\n Naming theme used to name the jobhandler and jobs.\n\n\nrun_max\n Maximum number of jobs that are submitted at a time.\n\n\ndo_snapshot\n Turn on/off snapshot creation. This is needed to load jobhandler instances in interactive slurmy.\n\n\nuse_snapshot\n Load snapshot from disk instead of creating new jobhandler.\n\n\ndescription\n Description of jobhandler that is stored in the bookkeeping.\n\n\nwrapper\n Default run script wrapper used for the job setup.\n\n\n\n\nMember functions\n\n\nadd_job\n\n\nJobHandler.add_job(self, backend=None, run_script=None, run_args=None, success_func=None, finished_func=None, post_func=None, max_retries=None, output=None, tags=None, parent_tags=None, name=None)\n\n\n\n\nAdd a job to the list of jobs to be processed by the \nJobHandler\n.\n\n\n\n\nbackend\n Backend instance to be used by the job.\n\n\nrun_script\n The run script processed by the job. This can be a string specifying the content of the script or a the absolute path to an already existing script file.\n\n\nrun_args\n The arguments passed to the run script.\n\n\nsuccess_func\n Success function used for the job setup.\n\n\nfinished_func\n Finished function used for the job setup.\n\n\npost_func\n Post execution function used for the job setup.\n\n\nmax_retries\n Maximum number of retries that are attempted when job is failing.\n\n\noutput\n Output file of the job.\n\n\ntags\n List of tags attached to the job.\n\n\nparent_tags\n List of parent tags attached to the job.\n\n\nname\n Name of the job. This must be a string that conforms with the restrictions on class property names. Slurmy will make sure that job names stay unique, even if the same job name is set multiple times.\n\n\n\n\nReturns the job (Job).\n\n\ncancel_jobs\n\n\nJobHandler.cancel_jobs(self, tags=None, only_local=False, only_batch=False, make_snapshot=True)\n\n\n\n\nCancel running jobs.\n\n\n\n\ntags\n Tags of jobs that will be cancelled.\n\n\nonly_local\n Cancel only local jobs.\n\n\nonly_batch\n Cancel only batch jobs.\n\n\nmake_snapshot\n Make a snapshot after cancelling jobs.\n\n\n\n\ncheck_status\n\n\nJobHandler.check_status(self, force_success_check=False)\n\n\n\n\nCheck the status of the jobs.\n\n\n\n\nforce_success_check\n Force the success routine to be run, even if the job is already in a post-finished state.\n\n\n\n\nprint_summary\n\n\nJobHandler.print_summary(self, time_spent=None)\n\n\n\n\nPrint a summary of the job processing.\n\n\nreset\n\n\nJobHandler.reset(self, skip_jobs=False)\n\n\n\n\nReset the \nJobHandler\n session.\n\n\n\n\nskip_jobs\n Skip job reset.\n\n\n\n\nrun_jobs\n\n\nJobHandler.run_jobs(self, interval=5, retry=False)\n\n\n\n\nRun the job submission routine. Jobs will be submitted continuously until all of them have been processed.\n\n\n\n\ninterval\n The interval at which the job submission will be done (in seconds). Can also be set to -1 to start every submission cycle manually.\n\n\nretry\n Retry failed or cancelled jobs.\n\n\n\n\nsubmit_jobs\n\n\nJobHandler.submit_jobs(self, tags=None, make_snapshot=True, wait=True, retry=False)\n\n\n\n\nSubmit jobs according to the \nJobHandler\n configuration.\n\n\n\n\ntags\n Tags of jobs that will be submitted.\n\n\nmake_snapshot\n Make a snapshot of the jobs and the \nJobHandler\n after the submission cycle.\n\n\nwait\n Wait for locally submitted job.\n\n\nretry\n Retry failed or cancelled jobs.\n\n\n\n\nupdate_snapshot\n\n\nJobHandler.update_snapshot(self, skip_jobs=False)\n\n\n\n\nUpdate snapshots of the \nJobHandler\n and the associated Jobs on disk. Snaphots are only updated if something changed in the respective \nJobHandlerConfig\n or \nJobConfig\n.\n\n\n\n\nskip_jobs\n Skip the job snapshot update.",
            "title": "JobHandler"
        },
        {
            "location": "/classes/JobHandler/#jobhandler",
            "text": "JobHandler(name=None, backend=None, work_dir='', local_max=0, is_verbose=False, success_func=None, finished_func=None, max_retries=0, theme=<Theme.Lovecraft: 1>, run_max=None, do_snapshot=True, use_snapshot=False, description=None, wrapper=None)  Main handle to setup and submit jobs. Internally stores most information in the  JobHandlerConfig  class, which is stored on disk as a snapshot of the  JobHandler  session.   name  Name of the  JobHandler . Defines the base directory name of the session.  backend  Default backend instance used for the job setup.  work_dir  Path where the base directory is created.  local_max  Maximum number of local jobs that will be submitted at a time.  is_verbose  Increase verbosity of shell output.  success_func  Default success function used for the job setup.  finished_func  Default finished function used for the job setup.  max_retries  Maximum number of retries that are attempted for failing jobs.  theme  Naming theme used to name the jobhandler and jobs.  run_max  Maximum number of jobs that are submitted at a time.  do_snapshot  Turn on/off snapshot creation. This is needed to load jobhandler instances in interactive slurmy.  use_snapshot  Load snapshot from disk instead of creating new jobhandler.  description  Description of jobhandler that is stored in the bookkeeping.  wrapper  Default run script wrapper used for the job setup.",
            "title": "JobHandler"
        },
        {
            "location": "/classes/JobHandler/#member-functions",
            "text": "",
            "title": "Member functions"
        },
        {
            "location": "/classes/JobHandler/#add_job",
            "text": "JobHandler.add_job(self, backend=None, run_script=None, run_args=None, success_func=None, finished_func=None, post_func=None, max_retries=None, output=None, tags=None, parent_tags=None, name=None)  Add a job to the list of jobs to be processed by the  JobHandler .   backend  Backend instance to be used by the job.  run_script  The run script processed by the job. This can be a string specifying the content of the script or a the absolute path to an already existing script file.  run_args  The arguments passed to the run script.  success_func  Success function used for the job setup.  finished_func  Finished function used for the job setup.  post_func  Post execution function used for the job setup.  max_retries  Maximum number of retries that are attempted when job is failing.  output  Output file of the job.  tags  List of tags attached to the job.  parent_tags  List of parent tags attached to the job.  name  Name of the job. This must be a string that conforms with the restrictions on class property names. Slurmy will make sure that job names stay unique, even if the same job name is set multiple times.   Returns the job (Job).",
            "title": "add_job"
        },
        {
            "location": "/classes/JobHandler/#cancel_jobs",
            "text": "JobHandler.cancel_jobs(self, tags=None, only_local=False, only_batch=False, make_snapshot=True)  Cancel running jobs.   tags  Tags of jobs that will be cancelled.  only_local  Cancel only local jobs.  only_batch  Cancel only batch jobs.  make_snapshot  Make a snapshot after cancelling jobs.",
            "title": "cancel_jobs"
        },
        {
            "location": "/classes/JobHandler/#check_status",
            "text": "JobHandler.check_status(self, force_success_check=False)  Check the status of the jobs.   force_success_check  Force the success routine to be run, even if the job is already in a post-finished state.",
            "title": "check_status"
        },
        {
            "location": "/classes/JobHandler/#print_summary",
            "text": "JobHandler.print_summary(self, time_spent=None)  Print a summary of the job processing.",
            "title": "print_summary"
        },
        {
            "location": "/classes/JobHandler/#reset",
            "text": "JobHandler.reset(self, skip_jobs=False)  Reset the  JobHandler  session.   skip_jobs  Skip job reset.",
            "title": "reset"
        },
        {
            "location": "/classes/JobHandler/#run_jobs",
            "text": "JobHandler.run_jobs(self, interval=5, retry=False)  Run the job submission routine. Jobs will be submitted continuously until all of them have been processed.   interval  The interval at which the job submission will be done (in seconds). Can also be set to -1 to start every submission cycle manually.  retry  Retry failed or cancelled jobs.",
            "title": "run_jobs"
        },
        {
            "location": "/classes/JobHandler/#submit_jobs",
            "text": "JobHandler.submit_jobs(self, tags=None, make_snapshot=True, wait=True, retry=False)  Submit jobs according to the  JobHandler  configuration.   tags  Tags of jobs that will be submitted.  make_snapshot  Make a snapshot of the jobs and the  JobHandler  after the submission cycle.  wait  Wait for locally submitted job.  retry  Retry failed or cancelled jobs.",
            "title": "submit_jobs"
        },
        {
            "location": "/classes/JobHandler/#update_snapshot",
            "text": "JobHandler.update_snapshot(self, skip_jobs=False)  Update snapshots of the  JobHandler  and the associated Jobs on disk. Snaphots are only updated if something changed in the respective  JobHandlerConfig  or  JobConfig .   skip_jobs  Skip the job snapshot update.",
            "title": "update_snapshot"
        },
        {
            "location": "/classes/JobHandlerConfig/",
            "text": "JobHandlerConfig\n\n\nJobHandlerConfig(name=None, backend=None, work_dir='', local_max=0, is_verbose=False, success_func=None, finished_func=None, max_retries=0, theme=<Theme.Lovecraft: 1>, run_max=None, do_snapshot=True, wrapper=None)\n\n\n\n\nConfig class for the \nJobHandler\n class. Stores all necessary information to load the \nJobHandler\n session at a later time. All properties are assigned with a custom getter function, which keeps track of updates to the respective property (tracked with the \"update\" variable).\n\n\nArguments: see \nJobHandler\n class.",
            "title": "JobHandlerConfig"
        },
        {
            "location": "/classes/JobHandlerConfig/#jobhandlerconfig",
            "text": "JobHandlerConfig(name=None, backend=None, work_dir='', local_max=0, is_verbose=False, success_func=None, finished_func=None, max_retries=0, theme=<Theme.Lovecraft: 1>, run_max=None, do_snapshot=True, wrapper=None)  Config class for the  JobHandler  class. Stores all necessary information to load the  JobHandler  session at a later time. All properties are assigned with a custom getter function, which keeps track of updates to the respective property (tracked with the \"update\" variable).  Arguments: see  JobHandler  class.",
            "title": "JobHandlerConfig"
        },
        {
            "location": "/classes/JobContainer/",
            "text": "JobContainer\n\n\nJobContainer()\n\n\n\n\nContainer class which holds the jobs associated to a \nJobHandler\n session. Jobs are attached as properties to allow for easy access in interactive slurmy.\n\n\nProperties\n\n\nstatus_CANCELLED\n\n\nList jobs in status CANCELLED.\n\n\nstatus_CONFIGURED\n\n\nList jobs in status CONFIGURED.\n\n\nstatus_FAILURE\n\n\nList jobs in status FAILURE.\n\n\nstatus_FINISHED\n\n\nList jobs in status FINISHED.\n\n\nstatus_RUNNING\n\n\nList jobs in status RUNNING.\n\n\nstatus_SUCCESS\n\n\nList jobs in status SUCCESS.\n\n\nMember functions\n\n\nget\n\n\nJobContainer.get(self, tags=None)\n\n\n\n\nGet the list of jobs.\n\n\n\n\ntags\n Tags that jobs are filtered on.\n\n\n\n\nReturns list of jobs ([Job]).\n\n\nprint\n\n\nJobContainer.print(self, tags=None, states=None, print_summary=True)\n\n\n\n\nPrint the list of jobs and their current status.\n\n\n\n\ntags\n Tags that jobs should match with (single string or list of strings). If a job has any of the provided tags it will be printed.\n\n\nstates\n States that jobs should match with (single string or list of strings). If a job is in any of the provided states it will be printed.\n\n\nprint_summary\n Print overall summary as well.",
            "title": "JobContainer"
        },
        {
            "location": "/classes/JobContainer/#jobcontainer",
            "text": "JobContainer()  Container class which holds the jobs associated to a  JobHandler  session. Jobs are attached as properties to allow for easy access in interactive slurmy.",
            "title": "JobContainer"
        },
        {
            "location": "/classes/JobContainer/#properties",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/classes/JobContainer/#status_cancelled",
            "text": "List jobs in status CANCELLED.",
            "title": "status_CANCELLED"
        },
        {
            "location": "/classes/JobContainer/#status_configured",
            "text": "List jobs in status CONFIGURED.",
            "title": "status_CONFIGURED"
        },
        {
            "location": "/classes/JobContainer/#status_failure",
            "text": "List jobs in status FAILURE.",
            "title": "status_FAILURE"
        },
        {
            "location": "/classes/JobContainer/#status_finished",
            "text": "List jobs in status FINISHED.",
            "title": "status_FINISHED"
        },
        {
            "location": "/classes/JobContainer/#status_running",
            "text": "List jobs in status RUNNING.",
            "title": "status_RUNNING"
        },
        {
            "location": "/classes/JobContainer/#status_success",
            "text": "List jobs in status SUCCESS.",
            "title": "status_SUCCESS"
        },
        {
            "location": "/classes/JobContainer/#member-functions",
            "text": "",
            "title": "Member functions"
        },
        {
            "location": "/classes/JobContainer/#get",
            "text": "JobContainer.get(self, tags=None)  Get the list of jobs.   tags  Tags that jobs are filtered on.   Returns list of jobs ([Job]).",
            "title": "get"
        },
        {
            "location": "/classes/JobContainer/#print",
            "text": "JobContainer.print(self, tags=None, states=None, print_summary=True)  Print the list of jobs and their current status.   tags  Tags that jobs should match with (single string or list of strings). If a job has any of the provided tags it will be printed.  states  States that jobs should match with (single string or list of strings). If a job is in any of the provided states it will be printed.  print_summary  Print overall summary as well.",
            "title": "print"
        },
        {
            "location": "/classes/Job/",
            "text": "Job\n\n\nJob(config)\n\n\n\n\nJob\n class that holds the job configuration and status information. Internally stores most information in the \nJobConfig\n class, which is stored on disk as a snapshot of the \nJob\n. Jobs are not meant to be set up directly but rather via \nJobHandler\n.add_job().\n\n\n\n\nconfig\n The \nJobConfig\n instance that defines the initial job setup.\n\n\n\n\nProperties\n\n\nis_local\n\n\nReturns if the job is set to local processing or not (bool).\n\n\nlog\n\n\nOpen the job log file with less.\n\n\nReturns the log file path (str).\n\n\nname\n\n\nReturns the name of the job (str).\n\n\nparent_tags\n\n\nReturns the list of parent tags associated to this job ([str]).\n\n\nscript\n\n\nOpen the job script file with less.\n\n\nReturns the script file path (str).\n\n\ntags\n\n\nReturns the list of tags associated to this job ([str]).\n\n\nMember functions\n\n\nadd_tag\n\n\nJob.add_tag(self, tag, is_parent=False)\n\n\n\n\nAdd tag to be associated to the job.\n\n\n\n\ntag\n Tag to add to the job.\n\n\nis_parent\n Mark tag as parent.\n\n\n\n\nadd_tags\n\n\nJob.add_tags(self, tags, is_parent=False)\n\n\n\n\nAdd a list of tags to be associated to the job.\n\n\n\n\ntags\n List of tags to add to the job.\n\n\nis_parent\n Mark tags as parent.\n\n\n\n\ncancel\n\n\nJob.cancel(self, clear_retry=False)\n\n\n\n\nCancel the job.\n\n\n\n\nclear_retry\n Deactivate automatic retry mechanism\n\n\n\n\nReturns the job status (Status).\n\n\nedit_script\n\n\nJob.edit_script(self, editor=None)\n\n\n\n\nOpen the job's run script in an editor.\n\n\n\n\neditor\n Command line editor to use. If none is specified, the default editor according to the slurmy config is used.\n\n\n\n\nget_status\n\n\nJob.get_status(self, skip_eval=False, force_success_check=False)\n\n\n\n\nGet the status of the job.\n\n\n\n\nskip_eval\n Skip the status evaluation and just return the stored value.\n\n\nforce_success_check\n Force the success routine to be run, even if the job is already in a post-finished state.\n\n\n\n\nReturns the job status (Status).\n\n\nhas_tag\n\n\nJob.has_tag(self, tag)\n\n\n\n\nCheck if the job has a given tag.\n\n\nReturns if job has the tag (bool).\n\n\nhas_tags\n\n\nJob.has_tags(self, tags)\n\n\n\n\nCheck if the job has any tag of a given list of tags.\n\n\n\n\ntags\n Set of tags.\n\n\n\n\nReturns if job has any of the tags (bool).\n\n\nrerun\n\n\nJob.rerun(self, local=False)\n\n\n\n\nResets the job and submits it again.\n\n\n\n\nlocal\n Submit as a local job.\n\n\n\n\nreset\n\n\nJob.reset(self, reset_retries=True)\n\n\n\n\nReset the job.\n\n\n\n\nreset_retries\n Also reset number of retries attempted so far.\n\n\n\n\nset_local\n\n\nJob.set_local(self, is_local=True)\n\n\n\n\nSet the job to be local/not local. \nJob\n needs to be in CONFIGURED state.\n\n\n\n\nis_local\n Turn on/off local processing for the job.\n\n\n\n\nsubmit\n\n\nJob.submit(self)\n\n\n\n\nSubmit the job.\n\n\nReturns the job status (Status).\n\n\nupdate_snapshot\n\n\nJob.update_snapshot(self)\n\n\n\n\nUpdate the job snapshot on disk. Snaphot is only updated if something changed in the \nJobConfig\n.\n\n\nwait\n\n\nJob.wait(self)\n\n\n\n\nIf job is locally processing, wait for the process to finish.",
            "title": "Job"
        },
        {
            "location": "/classes/Job/#job",
            "text": "Job(config)  Job  class that holds the job configuration and status information. Internally stores most information in the  JobConfig  class, which is stored on disk as a snapshot of the  Job . Jobs are not meant to be set up directly but rather via  JobHandler .add_job().   config  The  JobConfig  instance that defines the initial job setup.",
            "title": "Job"
        },
        {
            "location": "/classes/Job/#properties",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/classes/Job/#is_local",
            "text": "Returns if the job is set to local processing or not (bool).",
            "title": "is_local"
        },
        {
            "location": "/classes/Job/#log",
            "text": "Open the job log file with less.  Returns the log file path (str).",
            "title": "log"
        },
        {
            "location": "/classes/Job/#name",
            "text": "Returns the name of the job (str).",
            "title": "name"
        },
        {
            "location": "/classes/Job/#parent_tags",
            "text": "Returns the list of parent tags associated to this job ([str]).",
            "title": "parent_tags"
        },
        {
            "location": "/classes/Job/#script",
            "text": "Open the job script file with less.  Returns the script file path (str).",
            "title": "script"
        },
        {
            "location": "/classes/Job/#tags",
            "text": "Returns the list of tags associated to this job ([str]).",
            "title": "tags"
        },
        {
            "location": "/classes/Job/#member-functions",
            "text": "",
            "title": "Member functions"
        },
        {
            "location": "/classes/Job/#add_tag",
            "text": "Job.add_tag(self, tag, is_parent=False)  Add tag to be associated to the job.   tag  Tag to add to the job.  is_parent  Mark tag as parent.",
            "title": "add_tag"
        },
        {
            "location": "/classes/Job/#add_tags",
            "text": "Job.add_tags(self, tags, is_parent=False)  Add a list of tags to be associated to the job.   tags  List of tags to add to the job.  is_parent  Mark tags as parent.",
            "title": "add_tags"
        },
        {
            "location": "/classes/Job/#cancel",
            "text": "Job.cancel(self, clear_retry=False)  Cancel the job.   clear_retry  Deactivate automatic retry mechanism   Returns the job status (Status).",
            "title": "cancel"
        },
        {
            "location": "/classes/Job/#edit_script",
            "text": "Job.edit_script(self, editor=None)  Open the job's run script in an editor.   editor  Command line editor to use. If none is specified, the default editor according to the slurmy config is used.",
            "title": "edit_script"
        },
        {
            "location": "/classes/Job/#get_status",
            "text": "Job.get_status(self, skip_eval=False, force_success_check=False)  Get the status of the job.   skip_eval  Skip the status evaluation and just return the stored value.  force_success_check  Force the success routine to be run, even if the job is already in a post-finished state.   Returns the job status (Status).",
            "title": "get_status"
        },
        {
            "location": "/classes/Job/#has_tag",
            "text": "Job.has_tag(self, tag)  Check if the job has a given tag.  Returns if job has the tag (bool).",
            "title": "has_tag"
        },
        {
            "location": "/classes/Job/#has_tags",
            "text": "Job.has_tags(self, tags)  Check if the job has any tag of a given list of tags.   tags  Set of tags.   Returns if job has any of the tags (bool).",
            "title": "has_tags"
        },
        {
            "location": "/classes/Job/#rerun",
            "text": "Job.rerun(self, local=False)  Resets the job and submits it again.   local  Submit as a local job.",
            "title": "rerun"
        },
        {
            "location": "/classes/Job/#reset",
            "text": "Job.reset(self, reset_retries=True)  Reset the job.   reset_retries  Also reset number of retries attempted so far.",
            "title": "reset"
        },
        {
            "location": "/classes/Job/#set_local",
            "text": "Job.set_local(self, is_local=True)  Set the job to be local/not local.  Job  needs to be in CONFIGURED state.   is_local  Turn on/off local processing for the job.",
            "title": "set_local"
        },
        {
            "location": "/classes/Job/#submit",
            "text": "Job.submit(self)  Submit the job.  Returns the job status (Status).",
            "title": "submit"
        },
        {
            "location": "/classes/Job/#update_snapshot",
            "text": "Job.update_snapshot(self)  Update the job snapshot on disk. Snaphot is only updated if something changed in the  JobConfig .",
            "title": "update_snapshot"
        },
        {
            "location": "/classes/Job/#wait",
            "text": "Job.wait(self)  If job is locally processing, wait for the process to finish.",
            "title": "wait"
        },
        {
            "location": "/classes/JobConfig/",
            "text": "JobConfig\n\n\nJobConfig(backend, path, success_func=None, finished_func=None, post_func=None, max_retries=0, tags=None, parent_tags=None, is_local=False, output=None)\n\n\n\n\nConfig class for the \nJob\n class. Stores all necessary information to load the \nJob\n at a later time. All properties are assigned with a custom getter function, which keeps track of updates to the respective property (tracked with the \"update\" variable).\n\n\n\n\nbackend\n Backend instance used for the job setup.\n\n\npath\n Path of the job's snapshot file.\n\n\nsuccess_func\n Success function used for the job setup.\n\n\nfinished_func\n Finished function used for the job setup.\n\n\npost_func\n Post execution function used for the job setup.\n\n\nmax_retries\n Maximum number of retries that are attempted when job is failing.\n\n\ntags\n List of tags attached to the job.\n\n\nparent_tags\n List of parent tags attached to the job.\n\n\nis_local\n Define job as local.\n\n\noutput\n Output file of the job.",
            "title": "JobConfig"
        },
        {
            "location": "/classes/JobConfig/#jobconfig",
            "text": "JobConfig(backend, path, success_func=None, finished_func=None, post_func=None, max_retries=0, tags=None, parent_tags=None, is_local=False, output=None)  Config class for the  Job  class. Stores all necessary information to load the  Job  at a later time. All properties are assigned with a custom getter function, which keeps track of updates to the respective property (tracked with the \"update\" variable).   backend  Backend instance used for the job setup.  path  Path of the job's snapshot file.  success_func  Success function used for the job setup.  finished_func  Finished function used for the job setup.  post_func  Post execution function used for the job setup.  max_retries  Maximum number of retries that are attempted when job is failing.  tags  List of tags attached to the job.  parent_tags  List of parent tags attached to the job.  is_local  Define job as local.  output  Output file of the job.",
            "title": "JobConfig"
        },
        {
            "location": "/utils/SuccessOutputFile/",
            "text": "SuccessOutputFile\n\n\nSuccessOutputFile(delay=1)\n\n\n\n\nCallable class which can be used as success_func of a slurmy job. It checks if the output file that is associated to the job is present in the underlying file system.\n\n\n\n\ndelay\n Time (in seconds) that the job should wait before making the success evaluation.",
            "title": "SuccessOutputFile"
        },
        {
            "location": "/utils/SuccessOutputFile/#successoutputfile",
            "text": "SuccessOutputFile(delay=1)  Callable class which can be used as success_func of a slurmy job. It checks if the output file that is associated to the job is present in the underlying file system.   delay  Time (in seconds) that the job should wait before making the success evaluation.",
            "title": "SuccessOutputFile"
        },
        {
            "location": "/utils/SuccessTrigger/",
            "text": "SuccessTrigger\n\n\nSuccessTrigger(success_file, failure_file)\n\n\n\n\nCallable class which can be used as success_func of a slurmy job. It continuously checks if either the success_file or the failure_file is present in the underlying file system.\n\n\n\n\nsuccess_file\n The file which is created if the job is successful.\n\n\nfailure_file\n The file which is created if the job failed.",
            "title": "SuccessTrigger"
        },
        {
            "location": "/utils/SuccessTrigger/#successtrigger",
            "text": "SuccessTrigger(success_file, failure_file)  Callable class which can be used as success_func of a slurmy job. It continuously checks if either the success_file or the failure_file is present in the underlying file system.   success_file  The file which is created if the job is successful.  failure_file  The file which is created if the job failed.",
            "title": "SuccessTrigger"
        },
        {
            "location": "/utils/FinishedTrigger/",
            "text": "FinishedTrigger\n\n\nFinishedTrigger(finished_file)\n\n\n\n\nCallable class which can be used as finished_func of a slurmy job. It checks if finished_file is present in the underlying file system.\n\n\n\n\nfinished_file\n The file which is created if the job is finished.",
            "title": "FinishedTrigger"
        },
        {
            "location": "/utils/FinishedTrigger/#finishedtrigger",
            "text": "FinishedTrigger(finished_file)  Callable class which can be used as finished_func of a slurmy job. It checks if finished_file is present in the underlying file system.   finished_file  The file which is created if the job is finished.",
            "title": "FinishedTrigger"
        },
        {
            "location": "/utils/LogMover/",
            "text": "LogMover\n\n\nLogMover(target_path)\n\n\n\n\nCallable class which can be used as post_func of a slurmy job. It moves the slurm log file to the new destination target_path.\n\n\n\n\ntarget_path\n New destination of the log file.",
            "title": "LogMover"
        },
        {
            "location": "/utils/LogMover/#logmover",
            "text": "LogMover(target_path)  Callable class which can be used as post_func of a slurmy job. It moves the slurm log file to the new destination target_path.   target_path  New destination of the log file.",
            "title": "LogMover"
        }
    ]
}