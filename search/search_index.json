{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SLURMY - Special handLer for Universal Running of Multiple jobs, Yes! Slurmy is a general batch submission module, which allows to define very general jobs to be run on batch system setups on linux computing clusters. Currently, only slurm is supported as backend, but further backends can easily be added. The definition of the job execution is done with a general shell execution script, as is used by most batch systems. In addition to the batch definition, jobs can also be dynamically executed locally, which allows for an arbitrary combination of batch and local jobs. Installation First off, it is very much recommended to use slurmy in python3. While it is compatible with python2, some features only work (properly) in python3. You can either install slurmy using pip or directly check out the git repository. pip Most likely you won't have root privileges on your machine, so you have to make a user pip installation: pip install --user slurmy Depending on your setup pip points to the python2 or python3 pip executable. In order to ensure that you're installing slurmy for python3 you can be more explicit: pip3 install --user slurmy This will install the slurmy module in ~/.local/lib/pythonX.Y/site-packages/ ( X.Y indicates the version of your python installation) and the slurmy executables in ~/.local/bin/ . In order to ensure that python properly finds the module and you can run the executables, you should add the respective paths to PYTHONPATH and PATH : MYPYTHONVERSION=python$(python3 -c \"from sys import version_info; print('{}.{}'.format(version_info.major, version_info.minor))\")\\ export PYTHONPATH=~/.local/lib/$MYPYTHONVERSION/site-packages:$PYTHONPATH\\ export PATH=~/.local/bin:$PATH github Clone the latest stable tag or (if you're brave) master branch locally: git clone https://github.com/Thomas-Maier/slurmy.git\\ LATESTTAG=$(git describe --abbrev=0 --tags)\\ git co $LATESTTAG Make sure to add the respository folder to PYTHONPATH and the bin/ folder to PATH ': ## Assuming that we are in the repository folder now\\ export PYTHONPATH=$PWD:$PYTHONPATH\\ export PATH=$PWD/bin:$PATH Also, take a look at the slurmy config setup .","title":"Slurmy"},{"location":"#slurmy-special-handler-for-universal-running-of-multiple-jobs-yes","text":"Slurmy is a general batch submission module, which allows to define very general jobs to be run on batch system setups on linux computing clusters. Currently, only slurm is supported as backend, but further backends can easily be added. The definition of the job execution is done with a general shell execution script, as is used by most batch systems. In addition to the batch definition, jobs can also be dynamically executed locally, which allows for an arbitrary combination of batch and local jobs.","title":"SLURMY - Special handLer for Universal Running of Multiple jobs, Yes!"},{"location":"#installation","text":"First off, it is very much recommended to use slurmy in python3. While it is compatible with python2, some features only work (properly) in python3. You can either install slurmy using pip or directly check out the git repository.","title":"Installation"},{"location":"#pip","text":"Most likely you won't have root privileges on your machine, so you have to make a user pip installation: pip install --user slurmy Depending on your setup pip points to the python2 or python3 pip executable. In order to ensure that you're installing slurmy for python3 you can be more explicit: pip3 install --user slurmy This will install the slurmy module in ~/.local/lib/pythonX.Y/site-packages/ ( X.Y indicates the version of your python installation) and the slurmy executables in ~/.local/bin/ . In order to ensure that python properly finds the module and you can run the executables, you should add the respective paths to PYTHONPATH and PATH : MYPYTHONVERSION=python$(python3 -c \"from sys import version_info; print('{}.{}'.format(version_info.major, version_info.minor))\")\\ export PYTHONPATH=~/.local/lib/$MYPYTHONVERSION/site-packages:$PYTHONPATH\\ export PATH=~/.local/bin:$PATH","title":"pip"},{"location":"#github","text":"Clone the latest stable tag or (if you're brave) master branch locally: git clone https://github.com/Thomas-Maier/slurmy.git\\ LATESTTAG=$(git describe --abbrev=0 --tags)\\ git co $LATESTTAG Make sure to add the respository folder to PYTHONPATH and the bin/ folder to PATH ': ## Assuming that we are in the repository folder now\\ export PYTHONPATH=$PWD:$PYTHONPATH\\ export PATH=$PWD/bin:$PATH Also, take a look at the slurmy config setup .","title":"github"},{"location":"howto/","text":"General Usage You can just write a piece of python code that imports the required slurmy classes (e.g. JobHandler and the backend class of your choice), defines jobs and calls the job submission. Job execution definitions can either be provided by already written batch shell scripts, or by defining the content of the shell script directly in your python code. For both cases, arguments that should be passed upon the execution to the scripts can also be specified. Below you'll find several examples of how your job configuration script can look like. You should also have a look at the interactive slurmy section to get an idea what you can do. Make sure to also take a look at the documentation of JobHandler , in particular JobHandler.add_job() . What you want to do before doing anything else You can (and should) specify a slurmy config file, which defines your default configuration of relevant slurmy properties. In particular you can define which batch system backend you want to use and how it should be configured. This safes you from having to specify this every single time you want to use slurmy. You just need to create a file ~/.slurmy with this content (which you might want to modify): bookkeeping = ~/.slurmy_bookkeeping workdir = ./ backend = Slurm editor = emacs -nw ## Slurm backend options Slurm.partition = lsschaile #Slurm.clusters = #Slurm.qos = #Slurm.exclude = #Slurm.mem = #Slurm.time = #Slurm.export = Snapshots By default, slurmy will do bookkeeping of past JobHandler sessions. The information is stored in a json file, which is defined in the slurmy config. Snapshot making can be deactivated by passing the respective argument to JobHandler . Snapshot making is very useful, in particular if you want to make use of interactive slurmy . Simple example from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Define the run script content run_script = \"\"\" echo \"hans\" \"\"\" ## Add a job jh.add_job(run_script = run_script) ## Run all jobs jh.run_jobs() With explicit backend definition If you don't create a slurmy config file specifying the batch system backend to use, or if you want to specify backends for each job, then you can do the following. from slurmy import JobHandler, Slurm ## Set up the backend slurm = Slurm(partition = 'lsschaile') ## Set up the JobHandler jh = JobHandler(backend = slurm) ## Define the run script content run_script = \"\"\" echo \"hans\" \"\"\" ## Add a job ### The backend can be individually set for each job slurm_job = Slurm(partition = 'lsschaile', mem = '6000mb') jh.add_job(backend = slurm_job, run_script = run_script) ## Run all jobs jh.run_jobs() Using an already existing script file (with optional arguments) You can also just use an existing script file. In this case you just specify the file path as run_script . You can also specify additional arguments to be passed to the run script. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Specify path to run script on disk run_script = '/path/to/run_script' ## Additional arguments to be passed to the run script run_args = 'hans horst' ## Add a job jh.add_job(run_script = run_script, run_args = run_args) ## Run all jobs jh.run_jobs() Chaining Jobs with Tags Jobs can be connected by adding tags and parent tags to them. Jobs with parent tags X,Y, and Z will only be executed if all jobs that have the tags X, Y, or Z have successfully finished. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Define the run script content of job 1 run_script_1 = \"\"\" sleep 10 echo \"hans\" \"\"\" ## Add job 1 with tag \"hans\" jh.add_job(run_script = run_script_1, tags = 'hans') ## Define the run script content of job 2 run_script_2 = \"\"\" echo \"horst\" \"\"\" ## Add job 2 with parent_tag \"hans\" jh.add_job(run_script = run_script_2, parent_tags = 'hans') ## Run all jobs jh.run_jobs() The tags and parent_tags arguments of jh.add_job() can also be a list of tags, in order to assign multiple tags to a job at once. Additional uses of tags Tags can also be used to just organise jobs. In interactive slurmy you can easily print out only jobs which have a specified tag via JobContainer.print() (i.e. jh.jobs.print(tags = 'hans') for the example above). Also, if you properly installed the tqdm module (see the recommended setup ), slurmy will keep track of the job progress for each tag separately in addition to the overall progress. Steering evaluation of jobs processing status By default, the exitcode of the job (either taken from the local process or from the batch system bookkeeping) is taken to determine if it finished and was successful or not. However, you can change how slurmy will evaluate whether the job is finished or was successful. Job listener First a word on what the default evaluation setup of slurmy is. In general, jobs are designed to do their status evaluations themselves, i.e. the JobHandler asks the job for it's status. This can get very performance heavy if this is connected to a request to the batch system accounting/bookkeeping. By default, slurmy runs a Listener to collect the job information from the batch system and set it's exitcode when it's finished. Keep in mind that setting a finished_func (see below) will deactivate the listener and possibly slow down the job submission cycle (if you do something processing intensive for the evaluation). Define output file for success evaluation You can define the output of each job explicitly to change the success evaluation to check for the output to be present. From a technical side, this sets up a Listener which checks if the defined output files exist and sets the associated jobs status to SUCCESS if it finds them. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler(output_max_attempts = 5) ## Define the run script content run_script = \"\"\" touch ~/hans.txt \"\"\" ## Add a job, specifying the output of the job. jh.add_job(run_script = run_script, output = '~/hans.txt') ## Run all jobs jh.run_jobs() The output_max_attempts argument of the JobHandler defines how many attempts are made to find the output file for a given job that is in FINISHED state. By default it is set to 5, in order to avoid delayed availability of the output file in the underlying file system. FINISHED and SUCCESS trigger in the run_script You can also set triggers in the run_script to indicate at which point in the job processing it should be considered as FINISHED and/or SUCCESS. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Define the run script content run_script = \"\"\" echo \"hans\" @SLURMY.FINISHED echo \"horst\" @SLURMY.SUCCESS \"\"\" ## Add a job jh.add_job(run_script = run_script) ## Run all jobs jh.run_jobs() From a technical point, @SLURMY.FINISHED and @SLURMY.SUCCESS are substituted by temporary files, which are created at these points in the script. The FINISHED/SUCCESS evaluation is then checking if these files exist. The file name associated to @SLURMY.SUCCESS is also set as output , if it's not already specified as add_job argument. Custom evaluations for success, finished, and post-execution You can define a custom finished condition by creating a dedicated class with __call__ defined. The function has to have exactly one argument, which is the config instance of the job. If finished_func is defined during add_job, the custom definition will be used instead of the default one during the finished evaluation. This example uses the predefined FinishedTrigger class, which checks if the specified file exists on disk in order to evaluate whether if the job finished or not. from slurmy import JobHandler, FinishedTrigger ## Set up finished evaluation class finished_file = '~/hans.txt' ft = FinishedTrigger(finished_file) ## Set up the JobHandler jh = JobHandler() ## Define the run script content run_script = \"\"\" touch {} \"\"\".format(finished_file) ## Add a job jh.add_job(run_script = run_script, finished_func = ft) ## Run all jobs jh.run_jobs() This will actually do the same as the @SLURM.FINISHED example above. PLEASE NOTE: If you only specify finished_func with a simple(fast) evaluation, it's likely that your job will fail. This is simply because by default, slurmy will still ask the batch system accounting for the exitcode of the job and it's very likely that it's not updated in time. The same is true if you only put @SLURM.FINISHED and not @SLURM.SUCCESS in your run_script . Only specifying a custom success_func (or only @SLURM.SUCCESS ) is generally fine, though. In the same way as finished_func , you can also define success_func , to evaluate if a job is successful, or post_func , to define a post-processing which will be done locally after the job's success evaluation was done. Regarding class definitions Due to technically reasons connected to the snapshot feature , your custom class definition must be known to python on your machine. The best way to ensure that is to make the definition known to python via PYTHONPATH. In principle you can just use a local function definition instead of a callable class if you don't want to use the snapshot feature. However, it is highly recommended to make use of it. Execute job payload in Singularity with run script wrappers It's not unlikely that you will need to run your code not in the OS of the batch system worker nodes, but rather in a singularity environment. Slurmy provides a wrapper that will ensure that the run script is executed inside singularity on the worker node. You only need to provide the path to the singularity image that should be used. from slurmy import JobHandler, SingularityWrapper ## Set up the JobHandler sw = SingularityWrapper('/path/to/singularity/image.img') jh = JobHandler(wrapper = sw) ## Define the run script content run_script = \"\"\" echo \"hans\" \"\"\" ## Add a job jh.add_job(run_script = run_script) ## Run all jobs jh.run_jobs()","title":"HowTo"},{"location":"howto/#general-usage","text":"You can just write a piece of python code that imports the required slurmy classes (e.g. JobHandler and the backend class of your choice), defines jobs and calls the job submission. Job execution definitions can either be provided by already written batch shell scripts, or by defining the content of the shell script directly in your python code. For both cases, arguments that should be passed upon the execution to the scripts can also be specified. Below you'll find several examples of how your job configuration script can look like. You should also have a look at the interactive slurmy section to get an idea what you can do. Make sure to also take a look at the documentation of JobHandler , in particular JobHandler.add_job() .","title":"General Usage"},{"location":"howto/#what-you-want-to-do-before-doing-anything-else","text":"You can (and should) specify a slurmy config file, which defines your default configuration of relevant slurmy properties. In particular you can define which batch system backend you want to use and how it should be configured. This safes you from having to specify this every single time you want to use slurmy. You just need to create a file ~/.slurmy with this content (which you might want to modify): bookkeeping = ~/.slurmy_bookkeeping workdir = ./ backend = Slurm editor = emacs -nw ## Slurm backend options Slurm.partition = lsschaile #Slurm.clusters = #Slurm.qos = #Slurm.exclude = #Slurm.mem = #Slurm.time = #Slurm.export =","title":"What you want to do before doing anything else"},{"location":"howto/#snapshots","text":"By default, slurmy will do bookkeeping of past JobHandler sessions. The information is stored in a json file, which is defined in the slurmy config. Snapshot making can be deactivated by passing the respective argument to JobHandler . Snapshot making is very useful, in particular if you want to make use of interactive slurmy .","title":"Snapshots"},{"location":"howto/#simple-example","text":"from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Define the run script content run_script = \"\"\" echo \"hans\" \"\"\" ## Add a job jh.add_job(run_script = run_script) ## Run all jobs jh.run_jobs()","title":"Simple example"},{"location":"howto/#with-explicit-backend-definition","text":"If you don't create a slurmy config file specifying the batch system backend to use, or if you want to specify backends for each job, then you can do the following. from slurmy import JobHandler, Slurm ## Set up the backend slurm = Slurm(partition = 'lsschaile') ## Set up the JobHandler jh = JobHandler(backend = slurm) ## Define the run script content run_script = \"\"\" echo \"hans\" \"\"\" ## Add a job ### The backend can be individually set for each job slurm_job = Slurm(partition = 'lsschaile', mem = '6000mb') jh.add_job(backend = slurm_job, run_script = run_script) ## Run all jobs jh.run_jobs()","title":"With explicit backend definition"},{"location":"howto/#using-an-already-existing-script-file-with-optional-arguments","text":"You can also just use an existing script file. In this case you just specify the file path as run_script . You can also specify additional arguments to be passed to the run script. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Specify path to run script on disk run_script = '/path/to/run_script' ## Additional arguments to be passed to the run script run_args = 'hans horst' ## Add a job jh.add_job(run_script = run_script, run_args = run_args) ## Run all jobs jh.run_jobs()","title":"Using an already existing script file (with optional arguments)"},{"location":"howto/#chaining-jobs-with-tags","text":"Jobs can be connected by adding tags and parent tags to them. Jobs with parent tags X,Y, and Z will only be executed if all jobs that have the tags X, Y, or Z have successfully finished. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Define the run script content of job 1 run_script_1 = \"\"\" sleep 10 echo \"hans\" \"\"\" ## Add job 1 with tag \"hans\" jh.add_job(run_script = run_script_1, tags = 'hans') ## Define the run script content of job 2 run_script_2 = \"\"\" echo \"horst\" \"\"\" ## Add job 2 with parent_tag \"hans\" jh.add_job(run_script = run_script_2, parent_tags = 'hans') ## Run all jobs jh.run_jobs() The tags and parent_tags arguments of jh.add_job() can also be a list of tags, in order to assign multiple tags to a job at once.","title":"Chaining Jobs with Tags"},{"location":"howto/#additional-uses-of-tags","text":"Tags can also be used to just organise jobs. In interactive slurmy you can easily print out only jobs which have a specified tag via JobContainer.print() (i.e. jh.jobs.print(tags = 'hans') for the example above). Also, if you properly installed the tqdm module (see the recommended setup ), slurmy will keep track of the job progress for each tag separately in addition to the overall progress.","title":"Additional uses of tags"},{"location":"howto/#steering-evaluation-of-jobs-processing-status","text":"By default, the exitcode of the job (either taken from the local process or from the batch system bookkeeping) is taken to determine if it finished and was successful or not. However, you can change how slurmy will evaluate whether the job is finished or was successful.","title":"Steering evaluation of jobs processing status"},{"location":"howto/#job-listener","text":"First a word on what the default evaluation setup of slurmy is. In general, jobs are designed to do their status evaluations themselves, i.e. the JobHandler asks the job for it's status. This can get very performance heavy if this is connected to a request to the batch system accounting/bookkeeping. By default, slurmy runs a Listener to collect the job information from the batch system and set it's exitcode when it's finished. Keep in mind that setting a finished_func (see below) will deactivate the listener and possibly slow down the job submission cycle (if you do something processing intensive for the evaluation).","title":"Job listener"},{"location":"howto/#define-output-file-for-success-evaluation","text":"You can define the output of each job explicitly to change the success evaluation to check for the output to be present. From a technical side, this sets up a Listener which checks if the defined output files exist and sets the associated jobs status to SUCCESS if it finds them. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler(output_max_attempts = 5) ## Define the run script content run_script = \"\"\" touch ~/hans.txt \"\"\" ## Add a job, specifying the output of the job. jh.add_job(run_script = run_script, output = '~/hans.txt') ## Run all jobs jh.run_jobs() The output_max_attempts argument of the JobHandler defines how many attempts are made to find the output file for a given job that is in FINISHED state. By default it is set to 5, in order to avoid delayed availability of the output file in the underlying file system.","title":"Define output file for success evaluation"},{"location":"howto/#finished-and-success-trigger-in-the-run_script","text":"You can also set triggers in the run_script to indicate at which point in the job processing it should be considered as FINISHED and/or SUCCESS. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Define the run script content run_script = \"\"\" echo \"hans\" @SLURMY.FINISHED echo \"horst\" @SLURMY.SUCCESS \"\"\" ## Add a job jh.add_job(run_script = run_script) ## Run all jobs jh.run_jobs() From a technical point, @SLURMY.FINISHED and @SLURMY.SUCCESS are substituted by temporary files, which are created at these points in the script. The FINISHED/SUCCESS evaluation is then checking if these files exist. The file name associated to @SLURMY.SUCCESS is also set as output , if it's not already specified as add_job argument.","title":"FINISHED and SUCCESS trigger in the run_script"},{"location":"howto/#custom-evaluations-for-success-finished-and-post-execution","text":"You can define a custom finished condition by creating a dedicated class with __call__ defined. The function has to have exactly one argument, which is the config instance of the job. If finished_func is defined during add_job, the custom definition will be used instead of the default one during the finished evaluation. This example uses the predefined FinishedTrigger class, which checks if the specified file exists on disk in order to evaluate whether if the job finished or not. from slurmy import JobHandler, FinishedTrigger ## Set up finished evaluation class finished_file = '~/hans.txt' ft = FinishedTrigger(finished_file) ## Set up the JobHandler jh = JobHandler() ## Define the run script content run_script = \"\"\" touch {} \"\"\".format(finished_file) ## Add a job jh.add_job(run_script = run_script, finished_func = ft) ## Run all jobs jh.run_jobs() This will actually do the same as the @SLURM.FINISHED example above. PLEASE NOTE: If you only specify finished_func with a simple(fast) evaluation, it's likely that your job will fail. This is simply because by default, slurmy will still ask the batch system accounting for the exitcode of the job and it's very likely that it's not updated in time. The same is true if you only put @SLURM.FINISHED and not @SLURM.SUCCESS in your run_script . Only specifying a custom success_func (or only @SLURM.SUCCESS ) is generally fine, though. In the same way as finished_func , you can also define success_func , to evaluate if a job is successful, or post_func , to define a post-processing which will be done locally after the job's success evaluation was done.","title":"Custom evaluations for success, finished, and post-execution"},{"location":"howto/#regarding-class-definitions","text":"Due to technically reasons connected to the snapshot feature , your custom class definition must be known to python on your machine. The best way to ensure that is to make the definition known to python via PYTHONPATH. In principle you can just use a local function definition instead of a callable class if you don't want to use the snapshot feature. However, it is highly recommended to make use of it.","title":"Regarding class definitions"},{"location":"howto/#execute-job-payload-in-singularity-with-run-script-wrappers","text":"It's not unlikely that you will need to run your code not in the OS of the batch system worker nodes, but rather in a singularity environment. Slurmy provides a wrapper that will ensure that the run script is executed inside singularity on the worker node. You only need to provide the path to the singularity image that should be used. from slurmy import JobHandler, SingularityWrapper ## Set up the JobHandler sw = SingularityWrapper('/path/to/singularity/image.img') jh = JobHandler(wrapper = sw) ## Define the run script content run_script = \"\"\" echo \"hans\" \"\"\" ## Add a job jh.add_job(run_script = run_script) ## Run all jobs jh.run_jobs()","title":"Execute job payload in Singularity with run script wrappers"},{"location":"interactive_slurmy/","text":"You can use the slurmy executable to start an interactive slurmy session, which allows to interact with past JobHandler sessions or start new ones. Usage from slurmy --help : usage: slurmy [-h] [-p PATH] [-c CONFIG] [-t] [--debug] Slurmy interactive optional arguments: -h, --help show this help message and exit -p PATH, --path PATH Path to the base folder of an existing JobHandler session. Directly loads the JobHandler as \"jh\". -c CONFIG, --config CONFIG Path to a job configuration file. -t Switch to start in test mode. --debug Run in debugging mode. If you prefer to use python2 (not recommended), you can also run the slurmy2 executable. If no argument is passed to the slurmy executable, it tries to load the latest session according to the bookkeeping and load it as jh . Example usage In general you can do everything in interactive slurmy that you can also do in python file which handles your job definition. On top of that you can easily inspect and manipulate an already existing JobHandler session. Just executing slurmy will bring up the latest JobHandler session: In [1]: jh Out[1]: MyAnalysis_1531405633 Every JobHandler has a member jobs which keeps track of all it's attached jobs: In [2]: jh.jobs Out[2]: Job \"ttbar\": CONFIGURED Job \"wjets\": CONFIGURED Job \"ww\": CONFIGURED Job \"data\": CONFIGURED ------------ CONFIGURED(4) As you can see, the JobHandler in this case has four jobs named \"data\", \"ttbar\", \"wjets\", and \"ww\", which is in the CONFIGURED state. Every job is attached as property to JobHandler .jobs, which provides a direct handle to access them: In [3]: jh.jobs.ww Out[3]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: CONFIGURED Tags: {'bkg', 'ww'} Alternatively, jobs can be accessed directly by name via the JobHandler itself: In [4]: jh['ww'] Out[4]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: CONFIGURED Tags: {'bkg', 'ww'} JobHandler .jobs also has a status_ property for every possible job status, which will print all jobs which currently are in this status: In [5]: jh.jobs.status_CONFIGURED Job \"wjets\": CONFIGURED Job \"data\": CONFIGURED Job \"ww\": CONFIGURED Job \"ttbar\": CONFIGURED In [6]: jh.jobs.status_RUNNING In [7]: As you've seen above, the job \"ww\" (and \"ttbar\" and \"wjets\" for that matter) has a tag \"bkg\", which was attached to the job via the tags option of JobHandler.add_job() . You can get the printout for jobs only tagged with \"bkg\" by calling the JobContainer.print() method: In [7]: jh.jobs.print(tags='bkg') Job \"wjets\": CONFIGURED Job \"ww\": CONFIGURED Job \"ttbar\": CONFIGURED ------------ CONFIGURED(3) In this example, all jobs are in the CONFIGURED state so we can run the job submission with JobHandler.run_jobs() : In [8]: jh.run_jobs() all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=3, F=1, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=2, F=1, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=0, F=1, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (3/0/3) failed (batch/local/all): (1/0/1) Time spent: 12.4 s You can see that this produces two different printouts. During the processing you'll get progress bars which indicate how many jobs are completed. On the very right of these progess bars you can also see how many jobs ended up in the SUCCESS(S), FAILED(F), or CANCELLED(C) state. You can also see that for each tag that is introduced with JobHandler.add_job() one progress bar is displayed, which keeps track of the jobs assigned with this tag. Slurmy will also evaluate the tag hierarchy dependent on how tags were assigned to jobs and order them accordingly in this printout. In this example, each job has it's own name as tag and \"ww\", \"ttbar\", and \"wjets\" have \"bkg\" as an additional tag. As you can see from the printout above, job \"ww\" ended up in FAILED state. In [9]: jh.jobs.ww Out[9]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: FAILED Tags: {'bkg', 'ww'} We can access the log file of the job directly with Job.log (which opens the log file with less ), in order to find out what went wrong: In [10]: jh.jobs.ww.log Usually, you probably want to fix your job configuration setup to fix a systematic problem in the job's run script creation. However, you can edit the run script directly: In [11]: jh.jobs.ww.edit_script() If any of the jobs ended up in FAILED or CANCELLED state, they can be retried by passing retry = True to run_jobs : In [12]: jh.run_jobs(retry = True) all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=4, F=0, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=3, F=0, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (4/0/4) Time spent: 2.5 s The job \"ww\" is now in SUCCESS state (from fixing the run script before retrying the job): In [13]: jh.jobs.ww Out[13]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: SUCCESS Tags: {'bkg', 'ww'} Finally, if you want to start from a clean slate you can reset the JobHandler completely: In [14]: jh.reset() In [15]: jh.run_jobs() all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=4, F=0, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=3, F=0, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (4/0/4) Time spent: 11.3 s In this case you actually might want to start again from the job configuration script that you wrote for your job submission. Have a look at the JobHandler and Job documentation to see what you can execute in interactive slurmy. Job configuration file The job definition file passed with -c is a convenient way to make job definitions. Inside the slurmy session, all necessary imports, like JobHandler and the backend classes, are already provided. This allows for skimmed down JobHandler setups that then can be further interacted with (you can omit import statements). As long as your definition file is \"flat\" (no encapsulated definitions), i.e. like the examples given in the HowTo section, you can pass it to interactive slurmy. Interactive slurmy functions The interactive slurmy session also defines a couple of functions. list_sessions list_sessions() List all available slurmy session according to the central bookkeeping. load load(name) Load a slurmy session by name. name Name of the slurmy session, as listed by list_sessions(). Returns jobhandler associated to the session (JobHandler). load_path load_path(path) Load a slurmy session by full path. path Full folder path of the slurmy session, as listed by list_sessions(). Returns jobhandler associated to the session (JobHandler). load_latest load_latest() Load the latest slurmy session according to central bookkeeping. Returns jobhandler associated to the session (JobHandler).","title":"Interactive Slurmy"},{"location":"interactive_slurmy/#example-usage","text":"In general you can do everything in interactive slurmy that you can also do in python file which handles your job definition. On top of that you can easily inspect and manipulate an already existing JobHandler session. Just executing slurmy will bring up the latest JobHandler session: In [1]: jh Out[1]: MyAnalysis_1531405633 Every JobHandler has a member jobs which keeps track of all it's attached jobs: In [2]: jh.jobs Out[2]: Job \"ttbar\": CONFIGURED Job \"wjets\": CONFIGURED Job \"ww\": CONFIGURED Job \"data\": CONFIGURED ------------ CONFIGURED(4) As you can see, the JobHandler in this case has four jobs named \"data\", \"ttbar\", \"wjets\", and \"ww\", which is in the CONFIGURED state. Every job is attached as property to JobHandler .jobs, which provides a direct handle to access them: In [3]: jh.jobs.ww Out[3]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: CONFIGURED Tags: {'bkg', 'ww'} Alternatively, jobs can be accessed directly by name via the JobHandler itself: In [4]: jh['ww'] Out[4]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: CONFIGURED Tags: {'bkg', 'ww'} JobHandler .jobs also has a status_ property for every possible job status, which will print all jobs which currently are in this status: In [5]: jh.jobs.status_CONFIGURED Job \"wjets\": CONFIGURED Job \"data\": CONFIGURED Job \"ww\": CONFIGURED Job \"ttbar\": CONFIGURED In [6]: jh.jobs.status_RUNNING In [7]: As you've seen above, the job \"ww\" (and \"ttbar\" and \"wjets\" for that matter) has a tag \"bkg\", which was attached to the job via the tags option of JobHandler.add_job() . You can get the printout for jobs only tagged with \"bkg\" by calling the JobContainer.print() method: In [7]: jh.jobs.print(tags='bkg') Job \"wjets\": CONFIGURED Job \"ww\": CONFIGURED Job \"ttbar\": CONFIGURED ------------ CONFIGURED(3) In this example, all jobs are in the CONFIGURED state so we can run the job submission with JobHandler.run_jobs() : In [8]: jh.run_jobs() all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=3, F=1, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=2, F=1, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=0, F=1, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (3/0/3) failed (batch/local/all): (1/0/1) Time spent: 12.4 s You can see that this produces two different printouts. During the processing you'll get progress bars which indicate how many jobs are completed. On the very right of these progess bars you can also see how many jobs ended up in the SUCCESS(S), FAILED(F), or CANCELLED(C) state. You can also see that for each tag that is introduced with JobHandler.add_job() one progress bar is displayed, which keeps track of the jobs assigned with this tag. Slurmy will also evaluate the tag hierarchy dependent on how tags were assigned to jobs and order them accordingly in this printout. In this example, each job has it's own name as tag and \"ww\", \"ttbar\", and \"wjets\" have \"bkg\" as an additional tag. As you can see from the printout above, job \"ww\" ended up in FAILED state. In [9]: jh.jobs.ww Out[9]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: FAILED Tags: {'bkg', 'ww'} We can access the log file of the job directly with Job.log (which opens the log file with less ), in order to find out what went wrong: In [10]: jh.jobs.ww.log Usually, you probably want to fix your job configuration setup to fix a systematic problem in the job's run script creation. However, you can edit the run script directly: In [11]: jh.jobs.ww.edit_script() If any of the jobs ended up in FAILED or CANCELLED state, they can be retried by passing retry = True to run_jobs : In [12]: jh.run_jobs(retry = True) all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=4, F=0, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=3, F=0, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (4/0/4) Time spent: 2.5 s The job \"ww\" is now in SUCCESS state (from fixing the run script before retrying the job): In [13]: jh.jobs.ww Out[13]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: SUCCESS Tags: {'bkg', 'ww'} Finally, if you want to start from a clean slate you can reset the JobHandler completely: In [14]: jh.reset() In [15]: jh.run_jobs() all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=4, F=0, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=3, F=0, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (4/0/4) Time spent: 11.3 s In this case you actually might want to start again from the job configuration script that you wrote for your job submission. Have a look at the JobHandler and Job documentation to see what you can execute in interactive slurmy.","title":"Example usage"},{"location":"interactive_slurmy/#job-configuration-file","text":"The job definition file passed with -c is a convenient way to make job definitions. Inside the slurmy session, all necessary imports, like JobHandler and the backend classes, are already provided. This allows for skimmed down JobHandler setups that then can be further interacted with (you can omit import statements). As long as your definition file is \"flat\" (no encapsulated definitions), i.e. like the examples given in the HowTo section, you can pass it to interactive slurmy.","title":"Job configuration file"},{"location":"interactive_slurmy/#interactive-slurmy-functions","text":"The interactive slurmy session also defines a couple of functions.","title":"Interactive slurmy functions"},{"location":"interactive_slurmy/#list_sessions","text":"list_sessions() List all available slurmy session according to the central bookkeeping.","title":"list_sessions"},{"location":"interactive_slurmy/#load","text":"load(name) Load a slurmy session by name. name Name of the slurmy session, as listed by list_sessions(). Returns jobhandler associated to the session (JobHandler).","title":"load"},{"location":"interactive_slurmy/#load_path","text":"load_path(path) Load a slurmy session by full path. path Full folder path of the slurmy session, as listed by list_sessions(). Returns jobhandler associated to the session (JobHandler).","title":"load_path"},{"location":"interactive_slurmy/#load_latest","text":"load_latest() Load the latest slurmy session according to central bookkeeping. Returns jobhandler associated to the session (JobHandler).","title":"load_latest"},{"location":"classes/Job/","text":"Job Job(config) Job class that holds the job configuration and status information. Internally stores most information in the JobConfig class, which is stored on disk as a snapshot of the Job . Jobs are not meant to be set up directly but rather via JobHandler .add_job(). config The JobConfig instance that defines the initial job setup. Properties exitcode Returns the exitcode of the job (str or int). If the exitcode in the config is still None, will get it from the backend first. id Returns the ID of the job (int). log Open the job log file with less. Returns the log file path (str). mode Returns the mode the job is currently in (Mode). The job can either be ACTIVE or PASSIVE. If it is ACTIVE, status update is done by the job itself, otherwise it's done externally. name Returns the name of the job (str). output Returns the output path of the job (str). parent_tags Returns the list of parent tags associated to this job (set(str)). script Open the job script file with less. Returns the script file path (str). starttime Returns the timestamp at which the job is started by the JobHandler (int). status Returns the status of the job (Status). tags Returns the list of tags associated to this job (set(str)). type Returns the type of the job (Type). Member functions add_tag Job.add_tag(self, tag, is_parent=False) Add tag to be associated to the job. tag Tag to add to the job. is_parent Mark tag as parent. add_tags Job.add_tags(self, tags, is_parent=False) Add a list of tags to be associated to the job. tags List of tags to add to the job. is_parent Mark tags as parent. cancel Job.cancel(self, clear_retry=False) Cancel the job. clear_retry Deactivate automatic retry mechanism Returns the job status (Status). complete Job.complete(self) Run the completion routine of the job. edit_script Job.edit_script(self, editor=None) Open the job's run script in an editor. editor Command line editor to use. If none is specified, the editor specified in $EDITOR is used and if this is not set, the default editor according to the slurmy config is used. get_mode Job.get_mode(self, status) Returns the mode the job is in while being in the specified status (Mode). status Status to return the job mode for. get_status Job.get_status(self, skip_eval=False, force_success_check=False) Get the status of the job. skip_eval Skip the status evaluation and just return the stored value. force_success_check Force the success routine to be run, even if the job is already in a post-finished state. This will not work if job is in PASSIVE mode (i.e. relies on a Listener to set it's status). Returns the job status (Status). has_tag Job.has_tag(self, tag) Check if the job has a given tag. Returns if job has the tag (bool). has_tags Job.has_tags(self, tags) Check if the job has any tag of a given list of tags. tags Set of tags. Returns if job has any of the tags (bool). reset Job.reset(self, reset_retries=True) Reset the job. reset_retries Also reset number of retries attempted so far. set_mode Job.set_mode(self, status, mode) Set the mode the job will be in while being in the specified status. status Status for which the mode is set. mode Mode that the job is set to for the given status. submit Job.submit(self) Submit the job. Returns the job status (Status). update_snapshot Job.update_snapshot(self) Update the job snapshot on disk. Snaphot is only updated if something changed in the JobConfig . wait Job.wait(self) If job is locally processing, wait for the process to finish.","title":"Job"},{"location":"classes/Job/#job","text":"Job(config) Job class that holds the job configuration and status information. Internally stores most information in the JobConfig class, which is stored on disk as a snapshot of the Job . Jobs are not meant to be set up directly but rather via JobHandler .add_job(). config The JobConfig instance that defines the initial job setup.","title":"Job"},{"location":"classes/Job/#properties","text":"","title":"Properties"},{"location":"classes/Job/#exitcode","text":"Returns the exitcode of the job (str or int). If the exitcode in the config is still None, will get it from the backend first.","title":"exitcode"},{"location":"classes/Job/#id","text":"Returns the ID of the job (int).","title":"id"},{"location":"classes/Job/#log","text":"Open the job log file with less. Returns the log file path (str).","title":"log"},{"location":"classes/Job/#mode","text":"Returns the mode the job is currently in (Mode). The job can either be ACTIVE or PASSIVE. If it is ACTIVE, status update is done by the job itself, otherwise it's done externally.","title":"mode"},{"location":"classes/Job/#name","text":"Returns the name of the job (str).","title":"name"},{"location":"classes/Job/#output","text":"Returns the output path of the job (str).","title":"output"},{"location":"classes/Job/#parent_tags","text":"Returns the list of parent tags associated to this job (set(str)).","title":"parent_tags"},{"location":"classes/Job/#script","text":"Open the job script file with less. Returns the script file path (str).","title":"script"},{"location":"classes/Job/#starttime","text":"Returns the timestamp at which the job is started by the JobHandler (int).","title":"starttime"},{"location":"classes/Job/#status","text":"Returns the status of the job (Status).","title":"status"},{"location":"classes/Job/#tags","text":"Returns the list of tags associated to this job (set(str)).","title":"tags"},{"location":"classes/Job/#type","text":"Returns the type of the job (Type).","title":"type"},{"location":"classes/Job/#member-functions","text":"","title":"Member functions"},{"location":"classes/Job/#add_tag","text":"Job.add_tag(self, tag, is_parent=False) Add tag to be associated to the job. tag Tag to add to the job. is_parent Mark tag as parent.","title":"add_tag"},{"location":"classes/Job/#add_tags","text":"Job.add_tags(self, tags, is_parent=False) Add a list of tags to be associated to the job. tags List of tags to add to the job. is_parent Mark tags as parent.","title":"add_tags"},{"location":"classes/Job/#cancel","text":"Job.cancel(self, clear_retry=False) Cancel the job. clear_retry Deactivate automatic retry mechanism Returns the job status (Status).","title":"cancel"},{"location":"classes/Job/#complete","text":"Job.complete(self) Run the completion routine of the job.","title":"complete"},{"location":"classes/Job/#edit_script","text":"Job.edit_script(self, editor=None) Open the job's run script in an editor. editor Command line editor to use. If none is specified, the editor specified in $EDITOR is used and if this is not set, the default editor according to the slurmy config is used.","title":"edit_script"},{"location":"classes/Job/#get_mode","text":"Job.get_mode(self, status) Returns the mode the job is in while being in the specified status (Mode). status Status to return the job mode for.","title":"get_mode"},{"location":"classes/Job/#get_status","text":"Job.get_status(self, skip_eval=False, force_success_check=False) Get the status of the job. skip_eval Skip the status evaluation and just return the stored value. force_success_check Force the success routine to be run, even if the job is already in a post-finished state. This will not work if job is in PASSIVE mode (i.e. relies on a Listener to set it's status). Returns the job status (Status).","title":"get_status"},{"location":"classes/Job/#has_tag","text":"Job.has_tag(self, tag) Check if the job has a given tag. Returns if job has the tag (bool).","title":"has_tag"},{"location":"classes/Job/#has_tags","text":"Job.has_tags(self, tags) Check if the job has any tag of a given list of tags. tags Set of tags. Returns if job has any of the tags (bool).","title":"has_tags"},{"location":"classes/Job/#reset","text":"Job.reset(self, reset_retries=True) Reset the job. reset_retries Also reset number of retries attempted so far.","title":"reset"},{"location":"classes/Job/#set_mode","text":"Job.set_mode(self, status, mode) Set the mode the job will be in while being in the specified status. status Status for which the mode is set. mode Mode that the job is set to for the given status.","title":"set_mode"},{"location":"classes/Job/#submit","text":"Job.submit(self) Submit the job. Returns the job status (Status).","title":"submit"},{"location":"classes/Job/#update_snapshot","text":"Job.update_snapshot(self) Update the job snapshot on disk. Snaphot is only updated if something changed in the JobConfig .","title":"update_snapshot"},{"location":"classes/Job/#wait","text":"Job.wait(self) If job is locally processing, wait for the process to finish.","title":"wait"},{"location":"classes/JobConfig/","text":"JobConfig JobConfig(backend, path, success_func=None, finished_func=None, post_func=None, max_retries=0, tags=None, parent_tags=None, job_type=<Type.BATCH: 0>, output=None, starttime=None) Config class for the Job class. Stores all necessary information to load the Job at a later time. All properties are assigned with a custom getter function, which keeps track of updates to the respective property (tracked with the \"update\" variable). backend Backend instance used for the job setup. path Path of the job's snapshot file. success_func Success function used for the job setup. finished_func Finished function used for the job setup. post_func Post execution function used for the job setup. max_retries Maximum number of retries that are attempted when job is failing. tags List of tags attached to the job. parent_tags List of parent tags attached to the job. job_type Define the type of the job. output Output file of the job. starttime Timestamp at which job is started by the JobHandler . Member functions add_tags JobConfig.add_tags(self, tags, is_parent=False) Add a list of tags to the tags associated to the job. tags Tags to be added. is_parent Tags are added as parent tags.","title":"JobConfig"},{"location":"classes/JobConfig/#jobconfig","text":"JobConfig(backend, path, success_func=None, finished_func=None, post_func=None, max_retries=0, tags=None, parent_tags=None, job_type=<Type.BATCH: 0>, output=None, starttime=None) Config class for the Job class. Stores all necessary information to load the Job at a later time. All properties are assigned with a custom getter function, which keeps track of updates to the respective property (tracked with the \"update\" variable). backend Backend instance used for the job setup. path Path of the job's snapshot file. success_func Success function used for the job setup. finished_func Finished function used for the job setup. post_func Post execution function used for the job setup. max_retries Maximum number of retries that are attempted when job is failing. tags List of tags attached to the job. parent_tags List of parent tags attached to the job. job_type Define the type of the job. output Output file of the job. starttime Timestamp at which job is started by the JobHandler .","title":"JobConfig"},{"location":"classes/JobConfig/#member-functions","text":"","title":"Member functions"},{"location":"classes/JobConfig/#add_tags","text":"JobConfig.add_tags(self, tags, is_parent=False) Add a list of tags to the tags associated to the job. tags Tags to be added. is_parent Tags are added as parent tags.","title":"add_tags"},{"location":"classes/JobContainer/","text":"JobContainer JobContainer() Container class which holds the jobs associated to a JobHandler session. Jobs are attached as properties to allow for easy access in interactive slurmy. Properties status_CANCELLED List jobs in status CANCELLED. status_CONFIGURED List jobs in status CONFIGURED. status_FAILED List jobs in status FAILED. status_FINISHED List jobs in status FINISHED. status_RUNNING List jobs in status RUNNING. status_SUCCESS List jobs in status SUCCESS. Member functions get JobContainer.get(self, tags=None, states=None) Get the list of jobs. tags Tags that the jobs must match to (single string or list of strings). states Job states that the jobs must match to (single Status object or list of Status objects). Returns list of jobs ([Job]). print JobContainer.print(self, tags=None, states=None, print_summary=True) Print the list of jobs and their current status. tags Tags that jobs should match with (single string or list of strings). If a job has any of the provided tags it will be printed. states States that jobs should match with (single Status object or list of Status objects). If a job is in any of the provided states it will be printed. print_summary Print overall summary as well.","title":"JobContainer"},{"location":"classes/JobContainer/#jobcontainer","text":"JobContainer() Container class which holds the jobs associated to a JobHandler session. Jobs are attached as properties to allow for easy access in interactive slurmy.","title":"JobContainer"},{"location":"classes/JobContainer/#properties","text":"","title":"Properties"},{"location":"classes/JobContainer/#status_cancelled","text":"List jobs in status CANCELLED.","title":"status_CANCELLED"},{"location":"classes/JobContainer/#status_configured","text":"List jobs in status CONFIGURED.","title":"status_CONFIGURED"},{"location":"classes/JobContainer/#status_failed","text":"List jobs in status FAILED.","title":"status_FAILED"},{"location":"classes/JobContainer/#status_finished","text":"List jobs in status FINISHED.","title":"status_FINISHED"},{"location":"classes/JobContainer/#status_running","text":"List jobs in status RUNNING.","title":"status_RUNNING"},{"location":"classes/JobContainer/#status_success","text":"List jobs in status SUCCESS.","title":"status_SUCCESS"},{"location":"classes/JobContainer/#member-functions","text":"","title":"Member functions"},{"location":"classes/JobContainer/#get","text":"JobContainer.get(self, tags=None, states=None) Get the list of jobs. tags Tags that the jobs must match to (single string or list of strings). states Job states that the jobs must match to (single Status object or list of Status objects). Returns list of jobs ([Job]).","title":"get"},{"location":"classes/JobContainer/#print","text":"JobContainer.print(self, tags=None, states=None, print_summary=True) Print the list of jobs and their current status. tags Tags that jobs should match with (single string or list of strings). If a job has any of the provided tags it will be printed. states States that jobs should match with (single Status object or list of Status objects). If a job is in any of the provided states it will be printed. print_summary Print overall summary as well.","title":"print"},{"location":"classes/JobHandler/","text":"JobHandler JobHandler(name=None, backend=None, work_dir='', local_max=0, local_dynamic=False, verbosity=1, success_func=None, finished_func=None, max_retries=0, theme=<Theme.Lovecraft: 1>, run_max=None, do_snapshot=True, use_snapshot=False, description=None, wrapper=None, profiler=None, listens=True, output_max_attempts=5, printer_bar_mode=True) Main handle to setup and submit jobs. Internally stores most information in the JobHandlerConfig class, which is stored on disk as a snapshot of the JobHandler session. name Name of the JobHandler . Defines the base directory name of the session. backend Default backend instance used for the job setup. work_dir Path where the base directory is created. local_max Maximum number of local jobs that will be submitted at a time. local_dynamic Switch to dynamically allocate jobs to run locally, up to the local_max maximum. verbosity Verbosity of the shell output. success_func Default success function used for the job setup. finished_func Default finished function used for the job setup. max_retries Maximum number of retries that are attempted for failing jobs. theme Naming theme used to name the jobhandler and jobs. run_max Maximum number of jobs that are submitted at a time. do_snapshot Turn on/off snapshot creation. This is needed to load jobhandler instances in interactive slurmy. use_snapshot Load snapshot from disk instead of creating new jobhandler. description Description of jobhandler that is stored in the bookkeeping. wrapper Default run script wrapper used for the job setup. profiler Profiler to be used for profiling. printer_bar_mode Turn bar mode of the printer on/off. Member functions add_job JobHandler.add_job(self, backend=None, run_script=None, run_args=None, success_func=None, finished_func=None, post_func=None, max_retries=None, output=None, tags=None, parent_tags=None, name=None, job_type=<Type.BATCH: 0>, starttime=None) Add a job to the list of jobs to be processed by the JobHandler . backend Backend instance to be used by the job. run_script The run script processed by the job. This can be a string specifying the content of the script or a the absolute path to an already existing script file. run_args The arguments passed to the run script. success_func Success function used for the job setup. finished_func Finished function used for the job setup. post_func Post execution function used for the job setup. max_retries Maximum number of retries that are attempted when job is failing. output Output file of the job. This must be an absolute path. tags List of tags attached to the job. parent_tags List of parent tags attached to the job. name Name of the job. This must be a string that conforms with the restrictions on class property names. Slurmy will make sure that job names stay unique, even if the same job name is set multiple times. job_type Type of the job. Can be set to Type.LOCAL to make it a local processing job. starttime Timestamp at which job is started by the JobHandler . Returns the job (Job). cancel_jobs JobHandler.cancel_jobs(self, tags=None, only_local=False, only_batch=False, make_snapshot=True) Cancel running jobs. tags Tags of jobs that will be cancelled. only_local Cancel only local jobs. only_batch Cancel only batch jobs. make_snapshot Make a snapshot after cancelling jobs. check JobHandler.check(self, force_success_check=False, skip_eval=False, print_summary=True) Check the status and tags of the jobs. force_success_check Force the success routine to be run, even if the job is already in a post-finished state. print_summary Print the job processing summary. reset JobHandler.reset(self, skip_jobs=False) Reset the JobHandler session. skip_jobs Skip job reset. run_jobs JobHandler.run_jobs(self, interval=1, retry=False) Run the job submission routine. Jobs will be submitted continuously until all of them have been processed. interval The interval at which the job submission will be done (in seconds). Can also be set to -1 to start every submission cycle manually (will not work if Listeners are used). retry Retry jobs in status FAILED or CANCELLED. This will attempt one cycle of job retrying. set_jobs_config_attr JobHandler.set_jobs_config_attr(self, attr, val, tags=None, states=None) Set the job config attribute of jobs associated to the JobHandler . attr Name of the attribute which is set. val Value that the attribute is set to. tags Set of tags which the jobs have to match to. states Set of job states which the jobs have to match to. submit_jobs JobHandler.submit_jobs(self, tags=None, make_snapshot=True, wait=True, retry=False, skip_eval=False) Submit jobs according to the JobHandler configuration. tags Tags of jobs that will be submitted. make_snapshot Make a snapshot of the jobs and the JobHandler after the submission cycle. wait Wait for locally submitted job. retry Retry jobs in status FAILED or CANCELLED. This circumvents the automatic retry routine. skip_eval Skip job status evaluation everywhere. update_snapshot JobHandler.update_snapshot(self, skip_jobs=False) Update snapshots of the JobHandler and the associated Jobs on disk. Snaphots are only updated if something changed in the respective JobHandlerConfig or JobConfig . skip_jobs Skip the job snapshot update.","title":"JobHandler"},{"location":"classes/JobHandler/#jobhandler","text":"JobHandler(name=None, backend=None, work_dir='', local_max=0, local_dynamic=False, verbosity=1, success_func=None, finished_func=None, max_retries=0, theme=<Theme.Lovecraft: 1>, run_max=None, do_snapshot=True, use_snapshot=False, description=None, wrapper=None, profiler=None, listens=True, output_max_attempts=5, printer_bar_mode=True) Main handle to setup and submit jobs. Internally stores most information in the JobHandlerConfig class, which is stored on disk as a snapshot of the JobHandler session. name Name of the JobHandler . Defines the base directory name of the session. backend Default backend instance used for the job setup. work_dir Path where the base directory is created. local_max Maximum number of local jobs that will be submitted at a time. local_dynamic Switch to dynamically allocate jobs to run locally, up to the local_max maximum. verbosity Verbosity of the shell output. success_func Default success function used for the job setup. finished_func Default finished function used for the job setup. max_retries Maximum number of retries that are attempted for failing jobs. theme Naming theme used to name the jobhandler and jobs. run_max Maximum number of jobs that are submitted at a time. do_snapshot Turn on/off snapshot creation. This is needed to load jobhandler instances in interactive slurmy. use_snapshot Load snapshot from disk instead of creating new jobhandler. description Description of jobhandler that is stored in the bookkeeping. wrapper Default run script wrapper used for the job setup. profiler Profiler to be used for profiling. printer_bar_mode Turn bar mode of the printer on/off.","title":"JobHandler"},{"location":"classes/JobHandler/#member-functions","text":"","title":"Member functions"},{"location":"classes/JobHandler/#add_job","text":"JobHandler.add_job(self, backend=None, run_script=None, run_args=None, success_func=None, finished_func=None, post_func=None, max_retries=None, output=None, tags=None, parent_tags=None, name=None, job_type=<Type.BATCH: 0>, starttime=None) Add a job to the list of jobs to be processed by the JobHandler . backend Backend instance to be used by the job. run_script The run script processed by the job. This can be a string specifying the content of the script or a the absolute path to an already existing script file. run_args The arguments passed to the run script. success_func Success function used for the job setup. finished_func Finished function used for the job setup. post_func Post execution function used for the job setup. max_retries Maximum number of retries that are attempted when job is failing. output Output file of the job. This must be an absolute path. tags List of tags attached to the job. parent_tags List of parent tags attached to the job. name Name of the job. This must be a string that conforms with the restrictions on class property names. Slurmy will make sure that job names stay unique, even if the same job name is set multiple times. job_type Type of the job. Can be set to Type.LOCAL to make it a local processing job. starttime Timestamp at which job is started by the JobHandler . Returns the job (Job).","title":"add_job"},{"location":"classes/JobHandler/#cancel_jobs","text":"JobHandler.cancel_jobs(self, tags=None, only_local=False, only_batch=False, make_snapshot=True) Cancel running jobs. tags Tags of jobs that will be cancelled. only_local Cancel only local jobs. only_batch Cancel only batch jobs. make_snapshot Make a snapshot after cancelling jobs.","title":"cancel_jobs"},{"location":"classes/JobHandler/#check","text":"JobHandler.check(self, force_success_check=False, skip_eval=False, print_summary=True) Check the status and tags of the jobs. force_success_check Force the success routine to be run, even if the job is already in a post-finished state. print_summary Print the job processing summary.","title":"check"},{"location":"classes/JobHandler/#reset","text":"JobHandler.reset(self, skip_jobs=False) Reset the JobHandler session. skip_jobs Skip job reset.","title":"reset"},{"location":"classes/JobHandler/#run_jobs","text":"JobHandler.run_jobs(self, interval=1, retry=False) Run the job submission routine. Jobs will be submitted continuously until all of them have been processed. interval The interval at which the job submission will be done (in seconds). Can also be set to -1 to start every submission cycle manually (will not work if Listeners are used). retry Retry jobs in status FAILED or CANCELLED. This will attempt one cycle of job retrying.","title":"run_jobs"},{"location":"classes/JobHandler/#set_jobs_config_attr","text":"JobHandler.set_jobs_config_attr(self, attr, val, tags=None, states=None) Set the job config attribute of jobs associated to the JobHandler . attr Name of the attribute which is set. val Value that the attribute is set to. tags Set of tags which the jobs have to match to. states Set of job states which the jobs have to match to.","title":"set_jobs_config_attr"},{"location":"classes/JobHandler/#submit_jobs","text":"JobHandler.submit_jobs(self, tags=None, make_snapshot=True, wait=True, retry=False, skip_eval=False) Submit jobs according to the JobHandler configuration. tags Tags of jobs that will be submitted. make_snapshot Make a snapshot of the jobs and the JobHandler after the submission cycle. wait Wait for locally submitted job. retry Retry jobs in status FAILED or CANCELLED. This circumvents the automatic retry routine. skip_eval Skip job status evaluation everywhere.","title":"submit_jobs"},{"location":"classes/JobHandler/#update_snapshot","text":"JobHandler.update_snapshot(self, skip_jobs=False) Update snapshots of the JobHandler and the associated Jobs on disk. Snaphots are only updated if something changed in the respective JobHandlerConfig or JobConfig . skip_jobs Skip the job snapshot update.","title":"update_snapshot"},{"location":"classes/JobHandlerConfig/","text":"JobHandlerConfig JobHandlerConfig(name=None, backend=None, work_dir='', local_max=0, local_dynamic=False, success_func=None, finished_func=None, max_retries=0, theme=<Theme.Lovecraft: 1>, run_max=None, do_snapshot=True, wrapper=None, listens=True, output_max_attempts=5) Config class for the JobHandler class. Stores all necessary information to load the JobHandler session at a later time. All properties are assigned with a custom getter function, which keeps track of updates to the respective property (tracked with the \"update\" variable). Arguments: see JobHandler class.","title":"JobHandlerConfig"},{"location":"classes/JobHandlerConfig/#jobhandlerconfig","text":"JobHandlerConfig(name=None, backend=None, work_dir='', local_max=0, local_dynamic=False, success_func=None, finished_func=None, max_retries=0, theme=<Theme.Lovecraft: 1>, run_max=None, do_snapshot=True, wrapper=None, listens=True, output_max_attempts=5) Config class for the JobHandler class. Stores all necessary information to load the JobHandler session at a later time. All properties are assigned with a custom getter function, which keeps track of updates to the respective property (tracked with the \"update\" variable). Arguments: see JobHandler class.","title":"JobHandlerConfig"},{"location":"classes/Listener/","text":"Listener Listener(parent, listen_func, listen_status, map_property, max_attempts=None, fail_results=None) Listener class which is running in the background and collects state change information of jobs attached to the parent JobHandler instance. parent Parent JobHandler instance. listen_func The function definition used to collect the state change information. Results are collected in self._results as a dictionary with the keys being the defined map_property of the jobs. listen_status The job status the listener will consider. map_property Property name of jobs to which the respective job name is mapped to. The choice of the property name is driven by the definition of the listen_func. max_attempts Maximum number of attempts that the information collection will be executed. fail_results The results the job properties should be set to in case the max_attempts limit is reached. Member functions start Listener.start(self, interval=1) Spawn subprocess which continuously collects information by configurable mechanism and match any updates in the output to a state change decision of jobs. interval Interval at which information is collected by subprocess. stop Listener.stop(self) Stop listening subprocess. update_jobs Listener.update_jobs(self) Update jobs associated to parent JobHandler with the collected information.","title":"Listener"},{"location":"classes/Listener/#listener","text":"Listener(parent, listen_func, listen_status, map_property, max_attempts=None, fail_results=None) Listener class which is running in the background and collects state change information of jobs attached to the parent JobHandler instance. parent Parent JobHandler instance. listen_func The function definition used to collect the state change information. Results are collected in self._results as a dictionary with the keys being the defined map_property of the jobs. listen_status The job status the listener will consider. map_property Property name of jobs to which the respective job name is mapped to. The choice of the property name is driven by the definition of the listen_func. max_attempts Maximum number of attempts that the information collection will be executed. fail_results The results the job properties should be set to in case the max_attempts limit is reached.","title":"Listener"},{"location":"classes/Listener/#member-functions","text":"","title":"Member functions"},{"location":"classes/Listener/#start","text":"Listener.start(self, interval=1) Spawn subprocess which continuously collects information by configurable mechanism and match any updates in the output to a state change decision of jobs. interval Interval at which information is collected by subprocess.","title":"start"},{"location":"classes/Listener/#stop","text":"Listener.stop(self) Stop listening subprocess.","title":"stop"},{"location":"classes/Listener/#update_jobs","text":"Listener.update_jobs(self) Update jobs associated to parent JobHandler with the collected information.","title":"update_jobs"},{"location":"interactive_slurmy/preamble/","text":"You can use the slurmy executable to start an interactive slurmy session, which allows to interact with past JobHandler sessions or start new ones. Usage from slurmy --help : usage: slurmy [-h] [-p PATH] [-c CONFIG] [-t] [--debug] Slurmy interactive optional arguments: -h, --help show this help message and exit -p PATH, --path PATH Path to the base folder of an existing JobHandler session. Directly loads the JobHandler as \"jh\". -c CONFIG, --config CONFIG Path to a job configuration file. -t Switch to start in test mode. --debug Run in debugging mode. If you prefer to use python2 (not recommended), you can also run the slurmy2 executable. If no argument is passed to the slurmy executable, it tries to load the latest session according to the bookkeeping and load it as jh . Example usage In general you can do everything in interactive slurmy that you can also do in python file which handles your job definition. On top of that you can easily inspect and manipulate an already existing JobHandler session. Just executing slurmy will bring up the latest JobHandler session: In [1]: jh Out[1]: MyAnalysis_1531405633 Every JobHandler has a member jobs which keeps track of all it's attached jobs: In [2]: jh.jobs Out[2]: Job \"ttbar\": CONFIGURED Job \"wjets\": CONFIGURED Job \"ww\": CONFIGURED Job \"data\": CONFIGURED ------------ CONFIGURED(4) As you can see, the JobHandler in this case has four jobs named \"data\", \"ttbar\", \"wjets\", and \"ww\", which is in the CONFIGURED state. Every job is attached as property to JobHandler .jobs, which provides a direct handle to access them: In [3]: jh.jobs.ww Out[3]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: CONFIGURED Tags: {'bkg', 'ww'} Alternatively, jobs can be accessed directly by name via the JobHandler itself: In [4]: jh['ww'] Out[4]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: CONFIGURED Tags: {'bkg', 'ww'} JobHandler .jobs also has a status_ property for every possible job status, which will print all jobs which currently are in this status: In [5]: jh.jobs.status_CONFIGURED Job \"wjets\": CONFIGURED Job \"data\": CONFIGURED Job \"ww\": CONFIGURED Job \"ttbar\": CONFIGURED In [6]: jh.jobs.status_RUNNING In [7]: As you've seen above, the job \"ww\" (and \"ttbar\" and \"wjets\" for that matter) has a tag \"bkg\", which was attached to the job via the tags option of JobHandler.add_job() . You can get the printout for jobs only tagged with \"bkg\" by calling the JobContainer.print() method: In [7]: jh.jobs.print(tags='bkg') Job \"wjets\": CONFIGURED Job \"ww\": CONFIGURED Job \"ttbar\": CONFIGURED ------------ CONFIGURED(3) In this example, all jobs are in the CONFIGURED state so we can run the job submission with JobHandler.run_jobs() : In [8]: jh.run_jobs() all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=3, F=1, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=2, F=1, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=0, F=1, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (3/0/3) failed (batch/local/all): (1/0/1) Time spent: 12.4 s You can see that this produces two different printouts. During the processing you'll get progress bars which indicate how many jobs are completed. On the very right of these progess bars you can also see how many jobs ended up in the SUCCESS(S), FAILED(F), or CANCELLED(C) state. You can also see that for each tag that is introduced with JobHandler.add_job() one progress bar is displayed, which keeps track of the jobs assigned with this tag. Slurmy will also evaluate the tag hierarchy dependent on how tags were assigned to jobs and order them accordingly in this printout. In this example, each job has it's own name as tag and \"ww\", \"ttbar\", and \"wjets\" have \"bkg\" as an additional tag. As you can see from the printout above, job \"ww\" ended up in FAILED state. In [9]: jh.jobs.ww Out[9]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: FAILED Tags: {'bkg', 'ww'} We can access the log file of the job directly with Job.log (which opens the log file with less ), in order to find out what went wrong: In [10]: jh.jobs.ww.log Usually, you probably want to fix your job configuration setup to fix a systematic problem in the job's run script creation. However, you can edit the run script directly: In [11]: jh.jobs.ww.edit_script() If any of the jobs ended up in FAILED or CANCELLED state, they can be retried by passing retry = True to run_jobs : In [12]: jh.run_jobs(retry = True) all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=4, F=0, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=3, F=0, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (4/0/4) Time spent: 2.5 s The job \"ww\" is now in SUCCESS state (from fixing the run script before retrying the job): In [13]: jh.jobs.ww Out[13]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: SUCCESS Tags: {'bkg', 'ww'} Finally, if you want to start from a clean slate you can reset the JobHandler completely: In [14]: jh.reset() In [15]: jh.run_jobs() all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=4, F=0, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=3, F=0, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (4/0/4) Time spent: 11.3 s In this case you actually might want to start again from the job configuration script that you wrote for your job submission. Have a look at the JobHandler and Job documentation to see what you can execute in interactive slurmy. Job configuration file The job definition file passed with -c is a convenient way to make job definitions. Inside the slurmy session, all necessary imports, like JobHandler and the backend classes, are already provided. This allows for skimmed down JobHandler setups that then can be further interacted with (you can omit import statements). As long as your definition file is \"flat\" (no encapsulated definitions), i.e. like the examples given in the HowTo section, you can pass it to interactive slurmy. Interactive slurmy functions The interactive slurmy session also defines a couple of functions.","title":"Preamble"},{"location":"interactive_slurmy/preamble/#example-usage","text":"In general you can do everything in interactive slurmy that you can also do in python file which handles your job definition. On top of that you can easily inspect and manipulate an already existing JobHandler session. Just executing slurmy will bring up the latest JobHandler session: In [1]: jh Out[1]: MyAnalysis_1531405633 Every JobHandler has a member jobs which keeps track of all it's attached jobs: In [2]: jh.jobs Out[2]: Job \"ttbar\": CONFIGURED Job \"wjets\": CONFIGURED Job \"ww\": CONFIGURED Job \"data\": CONFIGURED ------------ CONFIGURED(4) As you can see, the JobHandler in this case has four jobs named \"data\", \"ttbar\", \"wjets\", and \"ww\", which is in the CONFIGURED state. Every job is attached as property to JobHandler .jobs, which provides a direct handle to access them: In [3]: jh.jobs.ww Out[3]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: CONFIGURED Tags: {'bkg', 'ww'} Alternatively, jobs can be accessed directly by name via the JobHandler itself: In [4]: jh['ww'] Out[4]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: CONFIGURED Tags: {'bkg', 'ww'} JobHandler .jobs also has a status_ property for every possible job status, which will print all jobs which currently are in this status: In [5]: jh.jobs.status_CONFIGURED Job \"wjets\": CONFIGURED Job \"data\": CONFIGURED Job \"ww\": CONFIGURED Job \"ttbar\": CONFIGURED In [6]: jh.jobs.status_RUNNING In [7]: As you've seen above, the job \"ww\" (and \"ttbar\" and \"wjets\" for that matter) has a tag \"bkg\", which was attached to the job via the tags option of JobHandler.add_job() . You can get the printout for jobs only tagged with \"bkg\" by calling the JobContainer.print() method: In [7]: jh.jobs.print(tags='bkg') Job \"wjets\": CONFIGURED Job \"ww\": CONFIGURED Job \"ttbar\": CONFIGURED ------------ CONFIGURED(3) In this example, all jobs are in the CONFIGURED state so we can run the job submission with JobHandler.run_jobs() : In [8]: jh.run_jobs() all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=3, F=1, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=2, F=1, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=0, F=1, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (3/0/3) failed (batch/local/all): (1/0/1) Time spent: 12.4 s You can see that this produces two different printouts. During the processing you'll get progress bars which indicate how many jobs are completed. On the very right of these progess bars you can also see how many jobs ended up in the SUCCESS(S), FAILED(F), or CANCELLED(C) state. You can also see that for each tag that is introduced with JobHandler.add_job() one progress bar is displayed, which keeps track of the jobs assigned with this tag. Slurmy will also evaluate the tag hierarchy dependent on how tags were assigned to jobs and order them accordingly in this printout. In this example, each job has it's own name as tag and \"ww\", \"ttbar\", and \"wjets\" have \"bkg\" as an additional tag. As you can see from the printout above, job \"ww\" ended up in FAILED state. In [9]: jh.jobs.ww Out[9]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: FAILED Tags: {'bkg', 'ww'} We can access the log file of the job directly with Job.log (which opens the log file with less ), in order to find out what went wrong: In [10]: jh.jobs.ww.log Usually, you probably want to fix your job configuration setup to fix a systematic problem in the job's run script creation. However, you can edit the run script directly: In [11]: jh.jobs.ww.edit_script() If any of the jobs ended up in FAILED or CANCELLED state, they can be retried by passing retry = True to run_jobs : In [12]: jh.run_jobs(retry = True) all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=4, F=0, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=3, F=0, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (4/0/4) Time spent: 2.5 s The job \"ww\" is now in SUCCESS state (from fixing the run script before retrying the job): In [13]: jh.jobs.ww Out[13]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: SUCCESS Tags: {'bkg', 'ww'} Finally, if you want to start from a clean slate you can reset the JobHandler completely: In [14]: jh.reset() In [15]: jh.run_jobs() all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=4, F=0, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=3, F=0, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (4/0/4) Time spent: 11.3 s In this case you actually might want to start again from the job configuration script that you wrote for your job submission. Have a look at the JobHandler and Job documentation to see what you can execute in interactive slurmy.","title":"Example usage"},{"location":"interactive_slurmy/preamble/#job-configuration-file","text":"The job definition file passed with -c is a convenient way to make job definitions. Inside the slurmy session, all necessary imports, like JobHandler and the backend classes, are already provided. This allows for skimmed down JobHandler setups that then can be further interacted with (you can omit import statements). As long as your definition file is \"flat\" (no encapsulated definitions), i.e. like the examples given in the HowTo section, you can pass it to interactive slurmy.","title":"Job configuration file"},{"location":"interactive_slurmy/preamble/#interactive-slurmy-functions","text":"The interactive slurmy session also defines a couple of functions.","title":"Interactive slurmy functions"},{"location":"utils/FinishedTrigger/","text":"FinishedTrigger FinishedTrigger(finished_file) Callable class which can be used as finished_func of a slurmy job. It checks if finished_file is present in the underlying file system. finished_file The file which is created if the job is finished.","title":"FinishedTrigger"},{"location":"utils/FinishedTrigger/#finishedtrigger","text":"FinishedTrigger(finished_file) Callable class which can be used as finished_func of a slurmy job. It checks if finished_file is present in the underlying file system. finished_file The file which is created if the job is finished.","title":"FinishedTrigger"},{"location":"utils/LogMover/","text":"LogMover LogMover(target_path) Callable class which can be used as post_func of a slurmy job. It moves the slurm log file to the new destination target_path. target_path New destination of the log file.","title":"LogMover"},{"location":"utils/LogMover/#logmover","text":"LogMover(target_path) Callable class which can be used as post_func of a slurmy job. It moves the slurm log file to the new destination target_path. target_path New destination of the log file.","title":"LogMover"},{"location":"utils/SuccessTrigger/","text":"SuccessTrigger SuccessTrigger(success_file, max_attempts) Callable class which can be used as success_func of a slurmy job. It checks if the success_file is present in the underlying file system, once a second. If the maximum number of attempts are reached without finding the file, it returns FAILED. success_file The file which is created if the job is successful. max_attempts Maximum number of attempts that will be tried to find the success_file.","title":"SuccessTrigger"},{"location":"utils/SuccessTrigger/#successtrigger","text":"SuccessTrigger(success_file, max_attempts) Callable class which can be used as success_func of a slurmy job. It checks if the success_file is present in the underlying file system, once a second. If the maximum number of attempts are reached without finding the file, it returns FAILED. success_file The file which is created if the job is successful. max_attempts Maximum number of attempts that will be tried to find the success_file.","title":"SuccessTrigger"}]}