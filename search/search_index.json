{
    "docs": [
        {
            "location": "/",
            "text": "SLURMY - Special handLer for Universal Running of Multiple jobs, Yes!\n\n\nSlurmy is a general batch submission module, which allows to define very general jobs to be run on batch system setups on linux computing clusters. Currently, only slurm is supported as backend, but further backends can easily be added. The definition of the job execution is done with a general shell execution script, as is used by most batch systems. In addition to the batch definition, jobs can also be dynamically executed locally, which allows for an arbitrary combination of batch and local jobs.\n\n\nCurrently, slurmy is only compatible with python 3. This is due to the highly more robust subprocess handling and other smaller features. In the future, I'll make in attempt to work with python 2 as well but that would be to the detriment of some of said feature (when used in python 2).\n\n\nNOTE: This readme temporarily will contain some instruction on how to use slurmy. In the future a proper documentation will be added.\n\n\nRecommended Setup\n\n\nClone the latest stable tag or master branch locally:\n\n\ngit clone https://github.com/Thomas-Maier/slurmy.git\n\n\nMake sure to add the directory in which you cloned slurmy to PYTHONPATH, and the slurmy folder to PATH:\n\n\nexport PYTHONPATH=$PWD:$PYTHONPATH\n\n\nexport PATH=$PWD/slurmy:$PATH\n\n\nThis will make python aware of the slurmy module and you'll be able to execute the slurmy executable.\n\n\nGeneral Usage\n\n\nYou can just write a piece of python code that imports the needed slurmy classes (e.g. JobHandler and the backend class of your choice), defines jobs and calls the job submission. Job execution definitions can either be provided by already written batch shell scripts, or by defining the content of the shell script directly in your python code. For both cases, arguments that should be passed upon the execution to the scripts can also be specified.\n\n\nExample of simple slurm jobs definition:\n \nexamples/example_slurm_simple.py\n\n\nYou can also make use of the interactive slurmy functionality to load a job configuration, more on that later.\n\n\nSlurmy Configuration File\n\n\nSince you'll most likely have only one batch system with some required configurations, it's possible to set up a general configuration file which specifies the backend to be used and it's configuration. If you do so, no backend configuration has to be done when making the job definition. You can also define some general slurmy configurations in this file.\n\n\nTemplate of slurmy config file:\n \nexamples/slurmy_config\n\n\nCopy this file into your home directory (name must be \".slurmy\") and set configurations according to your requirements:\n\n\ncp examples/slurmy_config ~/.slurmy\n\n\nNOTE: All further examples assume that ~/.slurmy exists, which properly defines the backend and backend configurations.\n\n\nChaining Jobs with Tags\n\n\nJobs can be connected by adding tags and parent tags to them. Jobs with parent tags X,Y, and Z will only be executed if all jobs that have the tags X, Y, or Z have successfully finished.\n\n\nExample of job chaining with tags:\n \nexamples/example_chain.py\n\n\nCustom Success or Finished Conditions and Variable Substitution\n\n\nBy default, the exitcode of the job (either taken from the local process or from the batch system bookkeeping) is taken to determine if it was successful or not. However, you can define a custom success condition by creating a dedicated class with __call__ defined. The function has to have exactly one argument, which is the config instance of the job. If \nsuccess_func\n was defined during add_job, the custom definition will be used instead of the default one during the success evaluation.\n\n\nExample of success_func usage:\n \nexamples/example_success_func.py\n\n\nDue to technically reasons connected to the snapshot feature (see below), your custom class definition must be known to python on your machine. The best way to ensure that is to make the definition known to python via PYTHONPATH. In principle you can just use a local function definition instead of a callable class if you don't want to use the snapshot feature. However, it is highly recommended to make use of it.\n\n\nIn the same way as the custom success condition, a custom finished condition can be provided with the \nfinished_func\n argument.\n\n\nThe example uses SuccessOutputFile as defined in tools/utils.py. It also introduces a feature of slurmy, that allows to use the configuration of the JobHandler inside the shell script and some arguments of add_job (currently only \"output\"). This is done by some simple string parsing and substitution routine of any variable of the JobHandlerConfig. For now have a look at \ntools/JobHandler.py\n to see what can be used (all variables that don't start with \"_\").\n\n\nSnapshots\n\n\nBy default, slurmy will make snapshots of your JobHandler session, which allows to load your job submission session at a later time (in interactive slurmy). You can deactivate this feature by passing \"do_snaphot = False\" to the JobHandler construction. If deactivated, the submission session can't be properly loaded in interactive slurmy anymore.\n\n\nJobHandler Options\n\n\nArguments that can be passed to the JobHandler construction:\n\n\nname\n (default: None): Name used for the session. This defines the name of the base folder, where logs, submission scripts, etc. are stored.\n\n\nbackend\n (default: None): Default backend to be used.\n\n\nwork_dir\n (default: \"\"): Path to the work directory where the base folder is to be placed.\n\n\nlocal_max\n (default: 0): Maximum number of concurrent local processes to be run. Setting this to a value greater than 0 will trigger the usage of local processes, otherwise only batch jobs will be submitted.\n\n\nsuccess_func\n (default: None): Default success definition to be used.\n\n\nfinished_func\n (default: None): Default finished definition to be used.\n\n\nmax_retries\n (default: 0): Maximum number of retries that will be attempted.\n\n\ntheme\n (default: Lovecraft): Theme that is used by the name generator to name individual jobs and the base folder name. Priority is given to the \"name\" argument. Themes can be used as given by the Theme enums in tools/defs.py. If Boring is used, job names are simply the base folder name with an incrementing integer.\n\n\nrun_max\n (default: None): Maximum number of jobs that are submitted concurrently. Sometimes needed to not overload batch systems.\n\n\ndo_snapshot\n (default: True): Switch for snapshot deactivation.\n\n\ndescription\n (default: None): Description of the JobHandler. For bookkeeping purposes.\n\n\nwrapper\n (default: None): Wrapper that defines how the run_script is executed on the worker. Currently only SingularityWrapper available (see \nexamples/example_wrapper.py\n for more information).\n\n\nArguments for add_jobs\n\n\nArguments that can be passed to the add_jobs function of JobHandler:\n\n\nbackend\n (default: None): Backend to be used by the job.\n\n\nrun_script\n (default: None): The shell script that defines the job execution. Can either be the script as one string block, or the name of a script on disk.\n\n\nrun_args\n (default: None): Run arguments that should be passed to the shell script. Can be either a string or a list of strings. It is recommended to simply pass a string that reflects the same sequence that you would also write in a shell line command.\n\n\nsuccess_func\n (default: None): Success definition to be used by the job.\n\n\nfinished_func\n (default: None): Finished definition to be used by the job.\n\n\nmax_retries\n (default: None): Maximum number of retries that will be attempted by the job.\n\n\noutput\n (default: None): User defined output of the job. Can be anything, since it's not used by the job.\n\n\ntags\n (default: None): Tags that should be attached to the job. Can be a string or a list of strings.\n\n\nparent_tags\n (default: None): Parent tags that should be attached to the job. Can be a string or a list of strings.\n\n\nname\n (default: None): Name to identify the job with. Slurmy makes sure that each job has a unique name and adds an incrementing integer as suffix, if needed.",
            "title": "Slurmy"
        },
        {
            "location": "/#slurmy-special-handler-for-universal-running-of-multiple-jobs-yes",
            "text": "Slurmy is a general batch submission module, which allows to define very general jobs to be run on batch system setups on linux computing clusters. Currently, only slurm is supported as backend, but further backends can easily be added. The definition of the job execution is done with a general shell execution script, as is used by most batch systems. In addition to the batch definition, jobs can also be dynamically executed locally, which allows for an arbitrary combination of batch and local jobs.  Currently, slurmy is only compatible with python 3. This is due to the highly more robust subprocess handling and other smaller features. In the future, I'll make in attempt to work with python 2 as well but that would be to the detriment of some of said feature (when used in python 2).  NOTE: This readme temporarily will contain some instruction on how to use slurmy. In the future a proper documentation will be added.",
            "title": "SLURMY - Special handLer for Universal Running of Multiple jobs, Yes!"
        },
        {
            "location": "/#recommended-setup",
            "text": "Clone the latest stable tag or master branch locally:  git clone https://github.com/Thomas-Maier/slurmy.git  Make sure to add the directory in which you cloned slurmy to PYTHONPATH, and the slurmy folder to PATH:  export PYTHONPATH=$PWD:$PYTHONPATH  export PATH=$PWD/slurmy:$PATH  This will make python aware of the slurmy module and you'll be able to execute the slurmy executable.",
            "title": "Recommended Setup"
        },
        {
            "location": "/#general-usage",
            "text": "You can just write a piece of python code that imports the needed slurmy classes (e.g. JobHandler and the backend class of your choice), defines jobs and calls the job submission. Job execution definitions can either be provided by already written batch shell scripts, or by defining the content of the shell script directly in your python code. For both cases, arguments that should be passed upon the execution to the scripts can also be specified.  Example of simple slurm jobs definition:   examples/example_slurm_simple.py  You can also make use of the interactive slurmy functionality to load a job configuration, more on that later.",
            "title": "General Usage"
        },
        {
            "location": "/#slurmy-configuration-file",
            "text": "Since you'll most likely have only one batch system with some required configurations, it's possible to set up a general configuration file which specifies the backend to be used and it's configuration. If you do so, no backend configuration has to be done when making the job definition. You can also define some general slurmy configurations in this file.  Template of slurmy config file:   examples/slurmy_config  Copy this file into your home directory (name must be \".slurmy\") and set configurations according to your requirements:  cp examples/slurmy_config ~/.slurmy  NOTE: All further examples assume that ~/.slurmy exists, which properly defines the backend and backend configurations.",
            "title": "Slurmy Configuration File"
        },
        {
            "location": "/#chaining-jobs-with-tags",
            "text": "Jobs can be connected by adding tags and parent tags to them. Jobs with parent tags X,Y, and Z will only be executed if all jobs that have the tags X, Y, or Z have successfully finished.  Example of job chaining with tags:   examples/example_chain.py",
            "title": "Chaining Jobs with Tags"
        },
        {
            "location": "/#custom-success-or-finished-conditions-and-variable-substitution",
            "text": "By default, the exitcode of the job (either taken from the local process or from the batch system bookkeeping) is taken to determine if it was successful or not. However, you can define a custom success condition by creating a dedicated class with __call__ defined. The function has to have exactly one argument, which is the config instance of the job. If  success_func  was defined during add_job, the custom definition will be used instead of the default one during the success evaluation.  Example of success_func usage:   examples/example_success_func.py  Due to technically reasons connected to the snapshot feature (see below), your custom class definition must be known to python on your machine. The best way to ensure that is to make the definition known to python via PYTHONPATH. In principle you can just use a local function definition instead of a callable class if you don't want to use the snapshot feature. However, it is highly recommended to make use of it.  In the same way as the custom success condition, a custom finished condition can be provided with the  finished_func  argument.  The example uses SuccessOutputFile as defined in tools/utils.py. It also introduces a feature of slurmy, that allows to use the configuration of the JobHandler inside the shell script and some arguments of add_job (currently only \"output\"). This is done by some simple string parsing and substitution routine of any variable of the JobHandlerConfig. For now have a look at  tools/JobHandler.py  to see what can be used (all variables that don't start with \"_\").",
            "title": "Custom Success or Finished Conditions and Variable Substitution"
        },
        {
            "location": "/#snapshots",
            "text": "By default, slurmy will make snapshots of your JobHandler session, which allows to load your job submission session at a later time (in interactive slurmy). You can deactivate this feature by passing \"do_snaphot = False\" to the JobHandler construction. If deactivated, the submission session can't be properly loaded in interactive slurmy anymore.",
            "title": "Snapshots"
        },
        {
            "location": "/#jobhandler-options",
            "text": "Arguments that can be passed to the JobHandler construction:  name  (default: None): Name used for the session. This defines the name of the base folder, where logs, submission scripts, etc. are stored.  backend  (default: None): Default backend to be used.  work_dir  (default: \"\"): Path to the work directory where the base folder is to be placed.  local_max  (default: 0): Maximum number of concurrent local processes to be run. Setting this to a value greater than 0 will trigger the usage of local processes, otherwise only batch jobs will be submitted.  success_func  (default: None): Default success definition to be used.  finished_func  (default: None): Default finished definition to be used.  max_retries  (default: 0): Maximum number of retries that will be attempted.  theme  (default: Lovecraft): Theme that is used by the name generator to name individual jobs and the base folder name. Priority is given to the \"name\" argument. Themes can be used as given by the Theme enums in tools/defs.py. If Boring is used, job names are simply the base folder name with an incrementing integer.  run_max  (default: None): Maximum number of jobs that are submitted concurrently. Sometimes needed to not overload batch systems.  do_snapshot  (default: True): Switch for snapshot deactivation.  description  (default: None): Description of the JobHandler. For bookkeeping purposes.  wrapper  (default: None): Wrapper that defines how the run_script is executed on the worker. Currently only SingularityWrapper available (see  examples/example_wrapper.py  for more information).",
            "title": "JobHandler Options"
        },
        {
            "location": "/#arguments-for-add_jobs",
            "text": "Arguments that can be passed to the add_jobs function of JobHandler:  backend  (default: None): Backend to be used by the job.  run_script  (default: None): The shell script that defines the job execution. Can either be the script as one string block, or the name of a script on disk.  run_args  (default: None): Run arguments that should be passed to the shell script. Can be either a string or a list of strings. It is recommended to simply pass a string that reflects the same sequence that you would also write in a shell line command.  success_func  (default: None): Success definition to be used by the job.  finished_func  (default: None): Finished definition to be used by the job.  max_retries  (default: None): Maximum number of retries that will be attempted by the job.  output  (default: None): User defined output of the job. Can be anything, since it's not used by the job.  tags  (default: None): Tags that should be attached to the job. Can be a string or a list of strings.  parent_tags  (default: None): Parent tags that should be attached to the job. Can be a string or a list of strings.  name  (default: None): Name to identify the job with. Slurmy makes sure that each job has a unique name and adds an incrementing integer as suffix, if needed.",
            "title": "Arguments for add_jobs"
        },
        {
            "location": "/interactive_slurmy/",
            "text": "Interactive Slurmy\n\n\nYou can use the slurmy executable to start an interactive slurmy session, which allows to interact with past JobHandler sessions or start new ones.\n\n\nArguments that can be passed to the executable:\n\n\n-p PATH\n: Path to the base folder of a JobHandler session. Directly loads the JobHandler as \"jh\".\n\n\n-c CONFIG\n: Path to a job definition file. More details, see below.\n\n\n--debug\n: Run in debugging mode.\n\n\nThe job definition file passed with \n-c\n is a convenient way to make job definitions. Inside the slurmy session, all necessary imports, like JobHandler and the backend classes, are already provided. This allows for skimmed down JobHandler setups that then can be further interacted with.\n\n\nExample of a job definition file which can be passed to slurmy:\n \nexamples/example_interactive_config.py\n\n\nIf no argument is passed to the slurmy executable, it tries to load the latest session according to the bookkeeping.\n\n\nThe interactive slurmy session also defines a couple of functions:\n\n\nlist_sessions()\n: List all past JobHandler sessions with some information. Sessions are kept track of in a json file, which is defined in ~/.slurmy. They are either defined by the full path to the base folder on disk, or by the name as given in the list.\n\n\nload(name)\n: Load a JobHandler as given by the name in list_sessions().\n\n\nload_path(path)\n: Load a JobHandler as given by the path to the base folder (relative or absolute).\n\n\nload_latest()\n: Load the latest session according to the bookkeeping.",
            "title": "Interactive Slurmy"
        },
        {
            "location": "/interactive_slurmy/#interactive-slurmy",
            "text": "You can use the slurmy executable to start an interactive slurmy session, which allows to interact with past JobHandler sessions or start new ones.  Arguments that can be passed to the executable:  -p PATH : Path to the base folder of a JobHandler session. Directly loads the JobHandler as \"jh\".  -c CONFIG : Path to a job definition file. More details, see below.  --debug : Run in debugging mode.  The job definition file passed with  -c  is a convenient way to make job definitions. Inside the slurmy session, all necessary imports, like JobHandler and the backend classes, are already provided. This allows for skimmed down JobHandler setups that then can be further interacted with.  Example of a job definition file which can be passed to slurmy:   examples/example_interactive_config.py  If no argument is passed to the slurmy executable, it tries to load the latest session according to the bookkeeping.  The interactive slurmy session also defines a couple of functions:  list_sessions() : List all past JobHandler sessions with some information. Sessions are kept track of in a json file, which is defined in ~/.slurmy. They are either defined by the full path to the base folder on disk, or by the name as given in the list.  load(name) : Load a JobHandler as given by the name in list_sessions().  load_path(path) : Load a JobHandler as given by the path to the base folder (relative or absolute).  load_latest() : Load the latest session according to the bookkeeping.",
            "title": "Interactive Slurmy"
        },
        {
            "location": "/classes/JobHandler/",
            "text": "JobHandler\n\n\nJobHandler(name=None, backend=None, work_dir='', local_max=0, is_verbose=False, success_func=None, finished_func=None, max_retries=0, theme=<Theme.Lovecraft: 1>, run_max=None, do_snapshot=True, use_snapshot=False, description=None, wrapper=None)\n\n\n\n\nMain handle to setup and submit jobs. Internally stores most information in the \nJobHandlerConfig\n class, which is stored on disk as a snapshot of the \nJobHandler\n session.\n\n\n\n\nname\n Name of the \nJobHandler\n. Defines the base directory name of the session.\n\n\nbackend\n Default backend instance used for the job setup.\n\n\nwork_dir\n Path where the base directory is created.\n\n\nlocal_max\n Maximum number of local jobs that will be submitted at a time.\n\n\nis_verbose\n Increase verbosity of shell output.\n\n\nsuccess_func\n Default success function used for the job setup.\n\n\nfinished_func\n Default finished function used for the job setup.\n\n\nmax_retries\n Maximum number of retries that are attempted for failing jobs.\n\n\ntheme\n Naming theme used to name the jobhandler and jobs.\n\n\nrun_max\n Maximum number of jobs that are submitted at a time.\n\n\ndo_snapshot\n Turn on/off snapshot creation. This is needed to load jobhandler instances in interactive slurmy.\n\n\nuse_snapshot\n Load snapshot from disk instead of creating new jobhandler.\n\n\ndescription\n Description of jobhandler that is stored in the bookkeeping.\n\n\nwrapper\n Default run script wrapper used for the job setup.\n\n\n\n\nMember functions\n\n\nadd_job\n\n\nJobHandler.add_job(self, backend=None, run_script=None, run_args=None, success_func=None, finished_func=None, post_func=None, max_retries=None, output=None, tags=None, parent_tags=None, name=None)\n\n\n\n\nAdd a job to the list of jobs to be processed by the \nJobHandler\n.\n\n\n\n\nbackend\n Backend instance to be used by the job.\n\n\nrun_script\n The run script processed by the job. This can be a string specifying the content of the script or a the absolute path to an already existing script file.\n\n\nrun_args\n The arguments passed to the run script.\n\n\nsuccess_func\n Success function used for the job setup.\n\n\nfinished_func\n Finished function used for the job setup.\n\n\npost_func\n Post execution function used for the job setup.\n\n\nmax_retries\n Maximum number of retries that are attempted when job is failing.\n\n\noutput\n Output file of the job.\n\n\ntags\n List of tags attached to the job.\n\n\nparent_tags\n List of parent tags attached to the job.\n\n\nname\n Name of the job. This must be a string that conforms with the restrictions on class property names. Slurmy will make sure that job names stay unique, even if the same job name is set multiple times.\n\n\n\n\ncancel_jobs\n\n\nJobHandler.cancel_jobs(self, tags=None, only_local=False, only_batch=False, make_snapshot=True)\n\n\n\n\nCancel running jobs.\n\n\n\n\ntags\n Tags of jobs that will be cancelled.\n\n\nonly_local\n Cancel only local jobs.\n\n\nonly_batch\n Cancel only batch jobs.\n\n\nmake_snapshot\n Make a snapshot after cancelling jobs.\n\n\n\n\ncheck_status\n\n\nJobHandler.check_status(self, force_success_check=False)\n\n\n\n\nCheck the status of the jobs.\n\n\n\n\nforce_success_check\n Force the success routine to be run, even if the job is already in a post-finished state.\n\n\n\n\nget_jobs\n\n\nJobHandler.get_jobs(self, tags=None)\n\n\n\n\nGet the list of jobs.\n\n\n\n\ntags\n Tags that jobs are filtered on.\n\n\n\n\nprint_summary\n\n\nJobHandler.print_summary(self, time_spent=None)\n\n\n\n\nPrint a summary of the job processing.\n\n\nreset\n\n\nJobHandler.reset(self)\n\n\n\n\nReset the \nJobHandler\n session.\n\n\nrun_jobs\n\n\nJobHandler.run_jobs(self, interval=5, retry=False, rerun=False)\n\n\n\n\nRun the job submission routine. Jobs will be submitted continuously until all of them have been processed.\n\n\n\n\ninterval\n The interval at which the job submission will be done (in seconds). Can also be set to -1 to start every submission cycle manually.\n\n\nretry\n Retry failed or cancelled jobs.\n\n\nrerun\n Rerun all jobs.\n\n\n\n\nsubmit_jobs\n\n\nJobHandler.submit_jobs(self, tags=None, make_snapshot=True, wait=True, retry=False, rerun=False)\n\n\n\n\nSubmit jobs according to the \nJobHandler\n configuration.\n\n\n\n\ntags\n Tags of jobs that will be submitted.\n\n\nmake_snapshot\n Make a snapshot of the jobs and the \nJobHandler\n after the submission cycle.\n\n\nwait\n Wait for locally submitted job.\n\n\nretry\n Retry failed or cancelled jobs.\n\n\nrerun\n Rerun all jobs.\n\n\n\n\nupdate_snapshot\n\n\nJobHandler.update_snapshot(self, skip_jobs=False)\n\n\n\n\nUpdate snapshots of the \nJobHandler\n and the associated Jobs on disk.\n\n\n\n\nskip_jobs\n Skip the job snapshot update.",
            "title": "JobHandler"
        },
        {
            "location": "/classes/JobHandler/#jobhandler",
            "text": "JobHandler(name=None, backend=None, work_dir='', local_max=0, is_verbose=False, success_func=None, finished_func=None, max_retries=0, theme=<Theme.Lovecraft: 1>, run_max=None, do_snapshot=True, use_snapshot=False, description=None, wrapper=None)  Main handle to setup and submit jobs. Internally stores most information in the  JobHandlerConfig  class, which is stored on disk as a snapshot of the  JobHandler  session.   name  Name of the  JobHandler . Defines the base directory name of the session.  backend  Default backend instance used for the job setup.  work_dir  Path where the base directory is created.  local_max  Maximum number of local jobs that will be submitted at a time.  is_verbose  Increase verbosity of shell output.  success_func  Default success function used for the job setup.  finished_func  Default finished function used for the job setup.  max_retries  Maximum number of retries that are attempted for failing jobs.  theme  Naming theme used to name the jobhandler and jobs.  run_max  Maximum number of jobs that are submitted at a time.  do_snapshot  Turn on/off snapshot creation. This is needed to load jobhandler instances in interactive slurmy.  use_snapshot  Load snapshot from disk instead of creating new jobhandler.  description  Description of jobhandler that is stored in the bookkeeping.  wrapper  Default run script wrapper used for the job setup.",
            "title": "JobHandler"
        },
        {
            "location": "/classes/JobHandler/#member-functions",
            "text": "",
            "title": "Member functions"
        },
        {
            "location": "/classes/JobHandler/#add_job",
            "text": "JobHandler.add_job(self, backend=None, run_script=None, run_args=None, success_func=None, finished_func=None, post_func=None, max_retries=None, output=None, tags=None, parent_tags=None, name=None)  Add a job to the list of jobs to be processed by the  JobHandler .   backend  Backend instance to be used by the job.  run_script  The run script processed by the job. This can be a string specifying the content of the script or a the absolute path to an already existing script file.  run_args  The arguments passed to the run script.  success_func  Success function used for the job setup.  finished_func  Finished function used for the job setup.  post_func  Post execution function used for the job setup.  max_retries  Maximum number of retries that are attempted when job is failing.  output  Output file of the job.  tags  List of tags attached to the job.  parent_tags  List of parent tags attached to the job.  name  Name of the job. This must be a string that conforms with the restrictions on class property names. Slurmy will make sure that job names stay unique, even if the same job name is set multiple times.",
            "title": "add_job"
        },
        {
            "location": "/classes/JobHandler/#cancel_jobs",
            "text": "JobHandler.cancel_jobs(self, tags=None, only_local=False, only_batch=False, make_snapshot=True)  Cancel running jobs.   tags  Tags of jobs that will be cancelled.  only_local  Cancel only local jobs.  only_batch  Cancel only batch jobs.  make_snapshot  Make a snapshot after cancelling jobs.",
            "title": "cancel_jobs"
        },
        {
            "location": "/classes/JobHandler/#check_status",
            "text": "JobHandler.check_status(self, force_success_check=False)  Check the status of the jobs.   force_success_check  Force the success routine to be run, even if the job is already in a post-finished state.",
            "title": "check_status"
        },
        {
            "location": "/classes/JobHandler/#get_jobs",
            "text": "JobHandler.get_jobs(self, tags=None)  Get the list of jobs.   tags  Tags that jobs are filtered on.",
            "title": "get_jobs"
        },
        {
            "location": "/classes/JobHandler/#print_summary",
            "text": "JobHandler.print_summary(self, time_spent=None)  Print a summary of the job processing.",
            "title": "print_summary"
        },
        {
            "location": "/classes/JobHandler/#reset",
            "text": "JobHandler.reset(self)  Reset the  JobHandler  session.",
            "title": "reset"
        },
        {
            "location": "/classes/JobHandler/#run_jobs",
            "text": "JobHandler.run_jobs(self, interval=5, retry=False, rerun=False)  Run the job submission routine. Jobs will be submitted continuously until all of them have been processed.   interval  The interval at which the job submission will be done (in seconds). Can also be set to -1 to start every submission cycle manually.  retry  Retry failed or cancelled jobs.  rerun  Rerun all jobs.",
            "title": "run_jobs"
        },
        {
            "location": "/classes/JobHandler/#submit_jobs",
            "text": "JobHandler.submit_jobs(self, tags=None, make_snapshot=True, wait=True, retry=False, rerun=False)  Submit jobs according to the  JobHandler  configuration.   tags  Tags of jobs that will be submitted.  make_snapshot  Make a snapshot of the jobs and the  JobHandler  after the submission cycle.  wait  Wait for locally submitted job.  retry  Retry failed or cancelled jobs.  rerun  Rerun all jobs.",
            "title": "submit_jobs"
        },
        {
            "location": "/classes/JobHandler/#update_snapshot",
            "text": "JobHandler.update_snapshot(self, skip_jobs=False)  Update snapshots of the  JobHandler  and the associated Jobs on disk.   skip_jobs  Skip the job snapshot update.",
            "title": "update_snapshot"
        },
        {
            "location": "/classes/JobHandlerConfig/",
            "text": "JobHandlerConfig\n\n\nJobHandlerConfig(name=None, backend=None, work_dir='', local_max=0, is_verbose=False, success_func=None, finished_func=None, max_retries=0, theme=<Theme.Lovecraft: 1>, run_max=None, do_snapshot=True, wrapper=None)\n\n\n\n\nConfig class for the \nJobHandler\n class. Stores all necessary information to load the \nJobHandler\n session at a later time. All properties are assigned with a custom getter function, which keeps track of updates to the respective property (tracked with the \"update\" variable).\n\n\nArguments: see \nJobHandler\n class.",
            "title": "JobHandlerConfig"
        },
        {
            "location": "/classes/JobHandlerConfig/#jobhandlerconfig",
            "text": "JobHandlerConfig(name=None, backend=None, work_dir='', local_max=0, is_verbose=False, success_func=None, finished_func=None, max_retries=0, theme=<Theme.Lovecraft: 1>, run_max=None, do_snapshot=True, wrapper=None)  Config class for the  JobHandler  class. Stores all necessary information to load the  JobHandler  session at a later time. All properties are assigned with a custom getter function, which keeps track of updates to the respective property (tracked with the \"update\" variable).  Arguments: see  JobHandler  class.",
            "title": "JobHandlerConfig"
        },
        {
            "location": "/classes/JobContainer/",
            "text": "JobContainer\n\n\nJobContainer()\n\n\n\n\nContainer class which holds the jobs associated to a \nJobHandler\n session. Jobs are attached as properties to allow for easy access in interactive slurmy.\n\n\nProperties\n\n\nstatus_CANCELLED\n\n\nList jobs in status CANCELLED.\n\n\nstatus_CONFIGURED\n\n\nList jobs in status CONFIGURED.\n\n\nstatus_FAILURE\n\n\nList jobs in status FAILURE.\n\n\nstatus_FINISHED\n\n\nList jobs in status FINISHED.\n\n\nstatus_RUNNING\n\n\nList jobs in status RUNNING.\n\n\nstatus_SUCCESS\n\n\nList jobs in status SUCCESS.",
            "title": "JobContainer"
        },
        {
            "location": "/classes/JobContainer/#jobcontainer",
            "text": "JobContainer()  Container class which holds the jobs associated to a  JobHandler  session. Jobs are attached as properties to allow for easy access in interactive slurmy.",
            "title": "JobContainer"
        },
        {
            "location": "/classes/JobContainer/#properties",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/classes/JobContainer/#status_cancelled",
            "text": "List jobs in status CANCELLED.",
            "title": "status_CANCELLED"
        },
        {
            "location": "/classes/JobContainer/#status_configured",
            "text": "List jobs in status CONFIGURED.",
            "title": "status_CONFIGURED"
        },
        {
            "location": "/classes/JobContainer/#status_failure",
            "text": "List jobs in status FAILURE.",
            "title": "status_FAILURE"
        },
        {
            "location": "/classes/JobContainer/#status_finished",
            "text": "List jobs in status FINISHED.",
            "title": "status_FINISHED"
        },
        {
            "location": "/classes/JobContainer/#status_running",
            "text": "List jobs in status RUNNING.",
            "title": "status_RUNNING"
        },
        {
            "location": "/classes/JobContainer/#status_success",
            "text": "List jobs in status SUCCESS.",
            "title": "status_SUCCESS"
        },
        {
            "location": "/classes/Job/",
            "text": "Job\n\n\nJob(config)\n\n\n\n\nJob\n class that holds the job configuration and status information. Internally stores most information in the \nJobConfig\n class, which is stored on disk as a snapshot of the \nJob\n. Jobs are not meant to be set up directly but rather via \nJobHandler\n.add_job().\n\n\n\n\nconfig\n The \nJobConfig\n instance that defines the initial job setup.\n\n\n\n\nProperties\n\n\nis_local\n\n\nReturns if the job is set to local processing or not.\n\n\nlog\n\n\nOpen the job log file with less.\n\n\nname\n\n\nReturn the name of the job.\n\n\nparent_tags\n\n\nReturn the list of parent tags associated to this job.\n\n\nscript\n\n\nOpen the job script file with less.\n\n\ntags\n\n\nReturn the list of tags associated to this job.\n\n\nMember functions\n\n\nadd_tag\n\n\nJob.add_tag(self, tag, is_parent=False)\n\n\n\n\nAdd tag to be associated to the job.\n\n\n\n\ntag\n Tag to add to the job.\n\n\nis_parent\n Mark tag as parent.\n\n\n\n\nadd_tags\n\n\nJob.add_tags(self, tags, is_parent=False)\n\n\n\n\nAdd a list of tags to be associated to the job.\n\n\n\n\ntags\n List of tags to add to the job.\n\n\nis_parent\n Mark tags as parent.\n\n\n\n\ncancel\n\n\nJob.cancel(self, clear_retry=False)\n\n\n\n\nCancel the job.\n\n\n\n\nclear_retry\n Deactivate automatic retry mechanism\n\n\n\n\nget_status\n\n\nJob.get_status(self, skip_eval=False, force_success_check=False)\n\n\n\n\nGet the status of the job.\n\n\n\n\nskip_eval\n Skip the status evaluation and just return the stored value.\n\n\nforce_success_check\n Force the success routine to be run, even if the job is already in a post-finished state.\n\n\n\n\nrerun\n\n\nJob.rerun(self, local=False)\n\n\n\n\nResets the job and submits it again.\n\n\n\n\nlocal\n Submit as a local job.\n\n\n\n\nreset\n\n\nJob.reset(self)\n\n\n\n\nReset the job.\n\n\nset_local\n\n\nJob.set_local(self, is_local=True)\n\n\n\n\nSet the job to be local/not local. \nJob\n needs to be in CONFIGURED state.\n\n\n\n\nis_local\n Turn on/off local processing for the job.\n\n\n\n\nsubmit\n\n\nJob.submit(self)\n\n\n\n\nSubmit the job.\n\n\nupdate_snapshot\n\n\nJob.update_snapshot(self)\n\n\n\n\nUpdate the job snapshot on disk. Snaphot is only updated if something changed in the \nJobConfig\n.\n\n\nwait\n\n\nJob.wait(self)\n\n\n\n\nIf job is locally processing, wait for the process to finish.",
            "title": "Job"
        },
        {
            "location": "/classes/Job/#job",
            "text": "Job(config)  Job  class that holds the job configuration and status information. Internally stores most information in the  JobConfig  class, which is stored on disk as a snapshot of the  Job . Jobs are not meant to be set up directly but rather via  JobHandler .add_job().   config  The  JobConfig  instance that defines the initial job setup.",
            "title": "Job"
        },
        {
            "location": "/classes/Job/#properties",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/classes/Job/#is_local",
            "text": "Returns if the job is set to local processing or not.",
            "title": "is_local"
        },
        {
            "location": "/classes/Job/#log",
            "text": "Open the job log file with less.",
            "title": "log"
        },
        {
            "location": "/classes/Job/#name",
            "text": "Return the name of the job.",
            "title": "name"
        },
        {
            "location": "/classes/Job/#parent_tags",
            "text": "Return the list of parent tags associated to this job.",
            "title": "parent_tags"
        },
        {
            "location": "/classes/Job/#script",
            "text": "Open the job script file with less.",
            "title": "script"
        },
        {
            "location": "/classes/Job/#tags",
            "text": "Return the list of tags associated to this job.",
            "title": "tags"
        },
        {
            "location": "/classes/Job/#member-functions",
            "text": "",
            "title": "Member functions"
        },
        {
            "location": "/classes/Job/#add_tag",
            "text": "Job.add_tag(self, tag, is_parent=False)  Add tag to be associated to the job.   tag  Tag to add to the job.  is_parent  Mark tag as parent.",
            "title": "add_tag"
        },
        {
            "location": "/classes/Job/#add_tags",
            "text": "Job.add_tags(self, tags, is_parent=False)  Add a list of tags to be associated to the job.   tags  List of tags to add to the job.  is_parent  Mark tags as parent.",
            "title": "add_tags"
        },
        {
            "location": "/classes/Job/#cancel",
            "text": "Job.cancel(self, clear_retry=False)  Cancel the job.   clear_retry  Deactivate automatic retry mechanism",
            "title": "cancel"
        },
        {
            "location": "/classes/Job/#get_status",
            "text": "Job.get_status(self, skip_eval=False, force_success_check=False)  Get the status of the job.   skip_eval  Skip the status evaluation and just return the stored value.  force_success_check  Force the success routine to be run, even if the job is already in a post-finished state.",
            "title": "get_status"
        },
        {
            "location": "/classes/Job/#rerun",
            "text": "Job.rerun(self, local=False)  Resets the job and submits it again.   local  Submit as a local job.",
            "title": "rerun"
        },
        {
            "location": "/classes/Job/#reset",
            "text": "Job.reset(self)  Reset the job.",
            "title": "reset"
        },
        {
            "location": "/classes/Job/#set_local",
            "text": "Job.set_local(self, is_local=True)  Set the job to be local/not local.  Job  needs to be in CONFIGURED state.   is_local  Turn on/off local processing for the job.",
            "title": "set_local"
        },
        {
            "location": "/classes/Job/#submit",
            "text": "Job.submit(self)  Submit the job.",
            "title": "submit"
        },
        {
            "location": "/classes/Job/#update_snapshot",
            "text": "Job.update_snapshot(self)  Update the job snapshot on disk. Snaphot is only updated if something changed in the  JobConfig .",
            "title": "update_snapshot"
        },
        {
            "location": "/classes/Job/#wait",
            "text": "Job.wait(self)  If job is locally processing, wait for the process to finish.",
            "title": "wait"
        },
        {
            "location": "/classes/JobConfig/",
            "text": "JobConfig\n\n\nJobConfig(backend, path, success_func=None, finished_func=None, post_func=None, max_retries=0, tags=None, parent_tags=None, is_local=False, output=None)\n\n\n\n\nConfig class for the \nJob\n class. Stores all necessary information to load the \nJob\n at a later time. All properties are assigned with a custom getter function, which keeps track of updates to the respective property (tracked with the \"update\" variable).\n\n\n\n\nbackend\n Backend instance used for the job setup.\n\n\npath\n Path of the job's snapshot file.\n\n\nsuccess_func\n Success function used for the job setup.\n\n\nfinished_func\n Finished function used for the job setup.\n\n\npost_func\n Post execution function used for the job setup.\n\n\nmax_retries\n Maximum number of retries that are attempted when job is failing.\n\n\ntags\n List of tags attached to the job.\n\n\nparent_tags\n List of parent tags attached to the job.\n\n\nis_local\n Define job as local.\n\n\noutput\n Output file of the job.",
            "title": "JobConfig"
        },
        {
            "location": "/classes/JobConfig/#jobconfig",
            "text": "JobConfig(backend, path, success_func=None, finished_func=None, post_func=None, max_retries=0, tags=None, parent_tags=None, is_local=False, output=None)  Config class for the  Job  class. Stores all necessary information to load the  Job  at a later time. All properties are assigned with a custom getter function, which keeps track of updates to the respective property (tracked with the \"update\" variable).   backend  Backend instance used for the job setup.  path  Path of the job's snapshot file.  success_func  Success function used for the job setup.  finished_func  Finished function used for the job setup.  post_func  Post execution function used for the job setup.  max_retries  Maximum number of retries that are attempted when job is failing.  tags  List of tags attached to the job.  parent_tags  List of parent tags attached to the job.  is_local  Define job as local.  output  Output file of the job.",
            "title": "JobConfig"
        }
    ]
}